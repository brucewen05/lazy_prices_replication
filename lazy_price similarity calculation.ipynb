{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "Sheet",
     "sheet_delimiter": true,
     "type": "MD"
    }
   },
   "source": [
    "# Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "tFfNzjjfvVhWtyjLz5RikR",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "vnzqVMrjo3ck3bq8qVuC0g",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "merged_data_2007_to_2012 = pd.read_csv(\"/data/workspace_files/lazy_price_replication/10k_final_with_ticker_name_filtered_w_similarity.csv\", index_col=0)\n",
    "ticker_prices = pd.read_csv(\"/data/workspace_files/lazy_price_replication/all_ticker_prices.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "yIGJJrFQoMiLuuXrq01QiK",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "data_sent  = pd.read_csv('/data/workspace_files/lazy_price_replication/10k_final_with_ticker_name_filtered_w_similarity_and_sentiment.csv')\n",
    "data_sent.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "90uo55bUrwI3nwjvJhcsMN",
     "type": "MD"
    }
   },
   "source": [
    "#### Data Description\n",
    "Each row represents the data for one ticker with the following information:\n",
    "1. cik: the cik of the stock\n",
    "2. report_period_end_date{year}: the ending period that the 10k report is for\n",
    "3. file_date{year}: the date the 10k report was filed and became public. NOTE: for a given report_period_end_date for the same year (e.g. 2007), the file_date could be next year (e.g. 2008)\n",
    "4. statement{year}: the actual content of the 10k statement\n",
    "5. company_name: the company's name\n",
    "6. sic: the sic for the compnay\n",
    "7. form_type: the given form type\n",
    "8. cusip_full: the 9 digit cusip for the compnay\n",
    "9. cusip: the 8 digit for the company\n",
    "10. ticker: the ticker name for the company"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "b1Fhp90oKe0FIqTDhrxa3n",
     "type": "MD"
    }
   },
   "source": [
    "## Consine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "fneDLv1FueMDExpsMYwpby",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "DXZDzB5CUIWRTVPgeaMoHa",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def consine_similarity(input1: str, input2: str)->float:\n",
    "    # Create a TfidfVectorizer to convert the text documents into TF-IDF vectors\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fit and transform the documents into TF-IDF matrix\n",
    "    tfidf_matrix = vectorizer.fit_transform([input1, input2])\n",
    "\n",
    "    # Compute the cosine similarity between the two documents\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "\n",
    "    # Extract the similarity score\n",
    "    similarity_score = similarity_matrix[0][0]\n",
    "    return similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "8tJq6DlXeKyDkfp8WcRPF5",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# Define the two documents as strings\n",
    "doc1 = \"We expect demand to increase\"\n",
    "doc2 = \"We expect worldwide demand to increase\"\n",
    "doc3 = \"We expect weakness in sales\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "prJC9vsBgKWbMwfq3mUKJV",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "similarity_score_1 = consine_similarity(doc1, doc3)\n",
    "print(f\"Cosine Similarity between the documents: {similarity_score_1}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "T2LFX8lSKPjGbC1Z6KqJRa",
     "type": "MD"
    }
   },
   "source": [
    "## Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "OT9ZS3bjFgqsgGGa0LvW1z",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "Jq1ln1gMIiOKGXsLj17Vsp",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(text: str)->str:\n",
    "    # Convert text to lowercase and remove non-alphanumeric characters\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def jaccard_similarity(input1: str, input2: str)->float:\n",
    "    # Preprocess and split documents into sets of unique words\n",
    "    set1 = set(preprocess(input1).split())\n",
    "    set2 = set(preprocess(input2).split())\n",
    "\n",
    "    # Calculate the intersection and union of the sets\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "\n",
    "    # Compute the Jaccard similarity\n",
    "    similarity = len(intersection) / len(union)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "XP60MHlwu6FbAl3xvVDRZa",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "jaccard_score_1 = jaccard_similarity(doc1, doc2)\n",
    "print(f\"Jaccard Similarity between the documents: {jaccard_score_1}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "awP0Llv1E85Tvi5rT0XzcH",
     "type": "MD"
    }
   },
   "source": [
    "## Min Edit Distance Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "0wqkb7un9HLDVZnKGMH0sh",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "hwl2wAbIqfAdotbpIjQj4V",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "distance = Levenshtein.distance(doc1, doc2)\n",
    "print(f\"Levenshtein Distance: {distance}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "HUtfCPwRVXRBKbgHGI82KM",
     "type": "MD"
    }
   },
   "source": [
    "### Populate similarity for different approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "ky7KYKMEA34U0QZLf7n6Tk",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "3EOTwNLCyUukb9lG5Bxco5",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "class SimilarityMethod(Enum):\n",
    "    COSINE = \"consine\"\n",
    "    JACCARD = \"jaccard\"\n",
    "    EDIT_DISTANCE = \"min_edit_distance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "NKsQBt9gYd8ipvIMfe3p13",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def compute_similary(similary_method: SimilarityMethod)->None:\n",
    "    years = list(range(2007, 2013))\n",
    "    for i in range(1, len(years)):\n",
    "        year1 = years[i-1]\n",
    "        year2 = years[i]\n",
    "        col_name = f\"{similary_method.value}_similarity_{year1}_to_{year2}\"\n",
    "        print(f\"processing {year1} to {year2} for {similary_method} with {col_name=}\")\n",
    "        # vals = []\n",
    "        if similary_method == SimilarityMethod.COSINE:\n",
    "            merged_data_2007_to_2012[col_name] = merged_data_2007_to_2012.apply(lambda row: consine_similarity(str(row[f\"statement{year1}\"]), str(row[f\"statement{year2}\"])), axis=1)\n",
    "        elif similary_method == SimilarityMethod.JACCARD:\n",
    "            merged_data_2007_to_2012[col_name] = merged_data_2007_to_2012.apply(lambda row: jaccard_similarity(str(row[f\"statement{year1}\"]), str(row[f\"statement{year2}\"])), axis=1)\n",
    "        elif similary_method.EDIT_DISTANCE:\n",
    "            merged_data_2007_to_2012[col_name] = merged_data_2007_to_2012.apply(lambda row: Levenshtein.distance(str(row[f\"statement{year1}\"]), str(row[f\"statement{year2}\"])), axis=1)\n",
    "        else:\n",
    "            raise Exception(f\"unsupported type: {similary_method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "8b63Mi1qQ3ZGkUIhXps7rT",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "compute_similary(SimilarityMethod.COSINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "qocUnYknlZ0YGft7uqIchh",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "compute_similary(SimilarityMethod.JACCARD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "ulippT9RmNWnxk77zY1sFX",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "compute_similary(SimilarityMethod.EDIT_DISTANCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "Wx9bMIKYkliPfJkYkTWWXT",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "data_sim = pd.read_csv('/data/workspace_files/lazy_price_replication/10k_final_unfiltered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "BIe2npmV7id3foUynlI6Je",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "data_sim.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "Ope3jgDkGAIEO8sdoyoutt",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "Sheet 2",
     "sheet_delimiter": true,
     "type": "MD"
    }
   },
   "source": [
    "# Sheet 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "KVQsbRiVX28tgqV4TGVmEU",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "0mtl2XFibV3awEZ40jyUqL",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "data_sim = pd.read_csv('/data/workspace_files/lazy_price_replication/10k_final_with_ticker_name_filtered_w_similarity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "citaQR0DsDYazIUWwbCSRw",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "text = data_sim['statement2007'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "GqXaSsaDeXtNOuK3Ke3ANb",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "data_sim = data_sim.dropna(subset=['ticker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "Zl8E9byNInPMlJUfqzTonC",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "date_columns = [f'file_date{year}' for year in range(2007, 2013)]\n",
    "\n",
    "# Initialize an empty list to store all the filtered dates\n",
    "all_dates = []\n",
    "\n",
    "# Loop through each file_date column\n",
    "for col in date_columns:\n",
    "    filings_date = data_sim[col].to_list()\n",
    "    date_obj = pd.to_datetime(filings_date, format='%Y%m%d', errors='coerce')\n",
    "    all_dates.extend(date_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "Gef0rDcb4Bl8HkKJ0IcIGo",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "date_columns = [f'report_period_end_date{year}' for year in range(2007, 2013)]\n",
    "\n",
    "# Initialize an empty list to store all the filtered dates\n",
    "all_dates_rep = []\n",
    "\n",
    "# Loop through each reporting_date column\n",
    "for col in date_columns:\n",
    "    filings_date = data_sim[col].to_list()\n",
    "    date_obj = pd.to_datetime(filings_date, format='%Y%m%d', errors='coerce')\n",
    "    all_dates_rep.extend(date_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "12njmojsGqrdr02TCCGjUb",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the list of dates into a pandas DataFrame for easier manipulation\n",
    "dates_df = pd.DataFrame(all_dates, columns=['date'])\n",
    "\n",
    "# Add a column for the year and the quarter\n",
    "dates_df['year'] = dates_df['date'].dt.year\n",
    "dates_df['quarter'] = dates_df['date'].dt.to_period('Q')  # This creates values like '2007Q1', '2007Q2', etc.\n",
    "\n",
    "# Count the number of dates in each quarter\n",
    "quarter_counts = dates_df['quarter'].value_counts().sort_index()\n",
    "\n",
    "# Plot the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "quarter_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title('Number of Filings per Quarter (2007-2012)', fontsize=16)\n",
    "plt.xlabel('Quarter', fontsize=12)\n",
    "plt.ylabel('Number of Dates', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "F9Pssj5lZu0025SKTssLZ6",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the list of dates into a pandas DataFrame for easier manipulation\n",
    "dates_df = pd.DataFrame(all_dates_rep, columns=['date'])\n",
    "\n",
    "# Add a column for the year and the quarter\n",
    "dates_df['year'] = dates_df['date'].dt.year\n",
    "dates_df['quarter'] = dates_df['date'].dt.to_period('Q')  # This creates values like '2007Q1', '2007Q2', etc.\n",
    "\n",
    "# Count the number of dates in each quarter\n",
    "quarter_counts = dates_df['quarter'].value_counts().sort_index()\n",
    "\n",
    "# Plot the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "quarter_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title('Reporting period per Quarter (2007-2012)', fontsize=16)\n",
    "plt.xlabel('Quarter', fontsize=12)\n",
    "plt.ylabel('Number of Dates', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "NW3ItRAKSEEjD1FziYitjT",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the years you want to analyze\n",
    "years = [2008, 2009, 2010, 2011, 2012]\n",
    "\n",
    "# Initialize an empty list to store the data dictionaries\n",
    "data_list = []\n",
    "\n",
    "# Loop through each year\n",
    "for year in years:\n",
    "    # Convert the 'file_date' column for the year to datetime format\n",
    "    data_sim[f'file_date{year}'] = pd.to_datetime(data_sim[f'file_date{year}'], format='%Y%m%d')\n",
    "    \n",
    "    # Define the start and end dates for each quarter and calculate trading dates\n",
    "    quarters = [\n",
    "        {'start': f'{year-1}-12-31', 'end': f'{year}-03-31'},  # Q1\n",
    "        {'start': f'{year}-04-01', 'end': f'{year}-06-30'},    # Q2\n",
    "        {'start': f'{year}-07-01', 'end': f'{year}-09-30'},    # Q3\n",
    "        {'start': f'{year}-10-01', 'end': f'{year}-12-31'},    # Q4\n",
    "    ]\n",
    "\n",
    "    # Loop through each quarter for the current year\n",
    "    for quarter in quarters:\n",
    "        start_date = pd.to_datetime(quarter['start'])\n",
    "        end_date = pd.to_datetime(quarter['end'])\n",
    "\n",
    "        # Calculate the trading start and end dates (next quarter)\n",
    "        if quarter['start'] == f'{year-1}-12-31':  # Q1\n",
    "            trading_start = pd.to_datetime(f'{year}-04-01')\n",
    "            trading_end = pd.to_datetime(f'{year}-06-30')\n",
    "            trading_quarter = 'Q2'\n",
    "            trading_year = year\n",
    "        elif quarter['start'] == f'{year}-04-01':  # Q2\n",
    "            trading_start = pd.to_datetime(f'{year}-07-01')\n",
    "            trading_end = pd.to_datetime(f'{year}-09-30')\n",
    "            trading_quarter = 'Q3'\n",
    "            trading_year = year\n",
    "        elif quarter['start'] == f'{year}-07-01':  # Q3\n",
    "            trading_start = pd.to_datetime(f'{year}-10-01')\n",
    "            trading_end = pd.to_datetime(f'{year}-12-31')\n",
    "            trading_quarter = 'Q4'\n",
    "            trading_year = year\n",
    "        else:  # Q4\n",
    "            trading_start = pd.to_datetime(f'{year+1}-01-01')\n",
    "            trading_end = pd.to_datetime(f'{year+1}-03-31')\n",
    "            trading_quarter = 'Q1'\n",
    "            trading_year = year + 1\n",
    "\n",
    "        # Filter the DataFrame for the specified time period\n",
    "        filtered_data = data_sim[(data_sim[f'file_date{year}'] >= start_date) & (data_sim[f'file_date{year}'] <= end_date)]\n",
    "        \n",
    "        # Sort the filtered data by similarity metrics\n",
    "        sorted_data_consine = filtered_data.sort_values(by=f'consine_similarity_{year-1}_to_{year}', ascending=True)\n",
    "        sorted_data_jac = filtered_data.sort_values(by=f'jaccard_similarity_{year-1}_to_{year}', ascending=True)\n",
    "        sorted_data_min_edit = filtered_data.sort_values(by=f'min_edit_distance_similarity_{year-1}_to_{year}', ascending=True)\n",
    "        \n",
    "        # Function to get the top and bottom quintiles\n",
    "        def get_quintiles(sorted_df, col_name):\n",
    "            quintile_size = int(np.ceil(len(sorted_df) * 0.20))\n",
    "            top_quintile = sorted_df.head(quintile_size)[col_name].to_list()\n",
    "            bottom_quintile = sorted_df.tail(quintile_size)[col_name].to_list()\n",
    "            return top_quintile, bottom_quintile\n",
    "\n",
    "        # Get the top and bottom quintile for each similarity measure\n",
    "        top_consine, bottom_consine = get_quintiles(sorted_data_consine, 'ticker')\n",
    "        top_jac, bottom_jac = get_quintiles(sorted_data_jac, 'ticker')\n",
    "        top_min_edit, bottom_min_edit = get_quintiles(sorted_data_min_edit, 'ticker')\n",
    "        # Determine the trading quarter based on the trading_start date\n",
    "\n",
    "        quarter_str = f\"{trading_start.year}-{trading_quarter}\"\n",
    "        # Create a dictionary to store the data for each quarter\n",
    "        data_dict = {\n",
    "            'year': year,\n",
    "            'quarter': quarter_str,\n",
    "            'start_date': start_date,\n",
    "            'end_date': end_date,\n",
    "            'trading_start': trading_start,\n",
    "            'trading_end': trading_end,\n",
    "            'Top Quintile Consine Similarity': top_consine,\n",
    "            'Bottom Quintile Consine Similarity': bottom_consine,\n",
    "            'Top Quintile Jaccard Similarity': top_jac,\n",
    "            'Bottom Quintile Jaccard Similarity': bottom_jac,\n",
    "            'Top Quintile Min Edit Distance Similarity': top_min_edit,\n",
    "            'Bottom Quintile Min Edit Distance Similarity': bottom_min_edit\n",
    "        }\n",
    "\n",
    "        # Append the dictionary to the list\n",
    "        data_list.append(data_dict)\n",
    "\n",
    "# Create a DataFrame from the list of dictionaries\n",
    "result_df = pd.DataFrame(data_list)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "UvkSmqPAWDidVyrQIVAxns",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "price_data = pd.read_csv('/data/workspace_files/lazy_price_replication/all_ticker_prices.csv', index_col=0)\n",
    "\n",
    "# Convert 'Date' to datetime\n",
    "price_data['Date'] = pd.to_datetime(price_data['Date'])\n",
    "\n",
    "# Extract year and quarter from 'Date'\n",
    "price_data['year'] = price_data['Date'].dt.year\n",
    "price_data['quarter'] = price_data['Date'].dt.quarter\n",
    "\n",
    "# Group by ticker, year, and quarter, then calculate quarterly return\n",
    "quarterly_returns = price_data.groupby(['ticker', 'year', 'quarter']).apply(\n",
    "    lambda x: (x['Close'].iloc[-1] - x['Close'].iloc[0]) / x['Close'].iloc[0]\n",
    ").reset_index(name='quarterly_return')\n",
    "\n",
    "# Create a new DataFrame with quarterly periods and returns\n",
    "quarterly_returns['quarter'] = quarterly_returns['year'].astype(str) + '-Q' + quarterly_returns['quarter'].astype(str)\n",
    "\n",
    "# Drop the 'year' column if it's not needed\n",
    "quarterly_prices = quarterly_returns[['ticker', 'quarter', 'quarterly_return']]\n",
    "\n",
    "# Display the new DataFrame\n",
    "price_df = quarterly_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "2TjF3w2plSMWzF03EdXAwz",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming results_df is your stock_df and returns_df is your quarterly_prices\n",
    "\n",
    "# Initialize an empty list to store portfolio returns\n",
    "portfolio_returns = []\n",
    "\n",
    "# Loop through each row of results_df to create the portfolio\n",
    "for index, row in result_df.iterrows():\n",
    "    # Get the tickers for long and short positions\n",
    "    long_stocks = row['Top Quintile Consine Similarity']\n",
    "    short_stocks = row['Bottom Quintile Consine Similarity']\n",
    "    \n",
    "    # Get the returns for the long stocks\n",
    "    long_returns = price_df[price_df['ticker'].isin(long_stocks) & (price_df['quarter'] == row['quarter'])]['quarterly_return']\n",
    "    \n",
    "    # Get the returns for the short stocks\n",
    "    short_returns = price_df[price_df['ticker'].isin(short_stocks) & (price_df['quarter'] == row['quarter'])]['quarterly_return']\n",
    "    \n",
    "    # Calculate the portfolio return for this quarter\n",
    "    if len(long_returns) > 0 and len(short_returns) > 0:\n",
    "        long_weight = 1 / len(long_stocks)  # Equal weight for long positions\n",
    "        short_weight = -1 / len(short_stocks)  # Equal weight for short positions\n",
    "        \n",
    "        # Portfolio return calculation\n",
    "        portfolio_return = (long_returns.sum() * long_weight) + (short_returns.sum() * short_weight)\n",
    "        portfolio_returns.append({\n",
    "            'quarter': row['quarter'],\n",
    "            'portfolio_return': portfolio_return\n",
    "        })\n",
    "\n",
    "# Create a DataFrame from the portfolio returns\n",
    "portfolio_df = pd.DataFrame(portfolio_returns)\n",
    "\n",
    "# Display the resulting portfolio returns DataFrame\n",
    "print(portfolio_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "noVc4xf0BhNSpLnfjevOl5",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get the tickers for long and short positions\n",
    "long_stocks = result_df['Top Quintile Jaccard Similarity'].iloc[0]\n",
    "short_stocks = result_df['Bottom Quintile Jaccard Similarity'].iloc[0]\n",
    "\n",
    "# Get the returns for the long stocks\n",
    "long_returns = price_df[price_df['ticker'].isin(long_stocks) & (price_df['quarter'] == result_df['quarter'].iloc[0])]['quarterly_return']\n",
    "\n",
    "# Get the returns for the short stocks\n",
    "short_returns = price_df[price_df['ticker'].isin(short_stocks) & (price_df['quarter'] == result_df['quarter'].iloc[0])]['quarterly_return']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "CGVu5r7i3vbKJ564Un8vA9",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "print(long_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "cg9iVqr4MeyY25ks24FTni",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample initial investment amount\n",
    "initial_investment = 10000  # or any amount you want\n",
    "\n",
    "# Calculate cumulative returns\n",
    "portfolio_df['cumulative_return'] = (1 + portfolio_df['portfolio_return']).cumprod() - 1\n",
    "\n",
    "# Calculate account value over time\n",
    "portfolio_df['account_value'] = initial_investment * (1 + portfolio_df['cumulative_return'])\n",
    "\n",
    "# Plotting the account value over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(portfolio_df['quarter'], portfolio_df['account_value'], marker='o', linestyle='-', color='blue')\n",
    "plt.title('Portfolio Account Value Over Time')\n",
    "plt.xlabel('Quarter')\n",
    "plt.ylabel('Account Value ($)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "gK1nszu6u4qtU3BFRmyhxd",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "(10389.210944304268/10000)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "AYm4Ix0bXSenuqnvQjM9sz",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "portfolio_df['account_value']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "JG space",
     "sheet_delimiter": true,
     "type": "MD"
    }
   },
   "source": [
    "# JG space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "0EwoHQXuauh69QZI6vcRCb",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "2hSs7PvV9Th9Jjo70NvHly",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "data_sim = pd.read_csv('/data/workspace_files/lazy_price_replication/10k_final_with_ticker_name_filtered_w_similarity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "nEdaM8e8D7DXTXiFufa1bP",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "data_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "i9ZPR8ITj8CFTEGfPLft5Q",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "date_columns = [f'file_date{year}' for year in range(2007, 2013)]\n",
    "\n",
    "# Initialize an empty list to store all the filtered dates\n",
    "all_dates = []\n",
    "\n",
    "# Loop through each file_date column\n",
    "for col in date_columns:\n",
    "    filings_date = data_sim[col].to_list()\n",
    "    date_obj = pd.to_datetime(filings_date, format='%Y%m%d', errors='coerce')\n",
    "    all_dates.extend(date_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "IfjKlIBfbzRVWzd5eSVJwo",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "date_columns = [f'report_period_end_date{year}' for year in range(2007, 2013)]\n",
    "\n",
    "# Initialize an empty list to store all the filtered dates\n",
    "all_dates_rep = []\n",
    "\n",
    "# Loop through each reporting_date column\n",
    "for col in date_columns:\n",
    "    filings_date = data_sim[col].to_list()\n",
    "    date_obj = pd.to_datetime(filings_date, format='%Y%m%d', errors='coerce')\n",
    "    all_dates_rep.extend(date_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "jDZbS0Lpq1QCCSJYCbFOrE",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "temp = dates_df['date'].dt.quarter.value_counts()\n",
    "labels = [f\"Q{quarter}\" for quarter in temp.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "LjMoWTaz1jn7Thr7OgDACG",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "temp = dates_df['date'].dt.quarter.value_counts()\n",
    "colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99']\n",
    "\n",
    "# Create the pie chart\n",
    "plt.figure(figsize=(6, 6))  # Size of the chart\n",
    "plt.pie(temp.values, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "\n",
    "# Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.axis('equal')\n",
    "\n",
    "# Add a title\n",
    "plt.title('Distribution of reports in every quarter')\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "rHy1K2cGZtP1iA9gh5UYc7",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the list of dates into a pandas DataFrame for easier manipulation\n",
    "dates_df = pd.DataFrame(all_dates, columns=['date'])\n",
    "\n",
    "# Add a column for the year and the quarter\n",
    "dates_df['year'] = dates_df['date'].dt.year\n",
    "dates_df['quarter'] = dates_df['date'].dt.to_period('Q')  # This creates values like '2007Q1', '2007Q2', etc.\n",
    "\n",
    "# Count the number of dates in each quarter\n",
    "quarter_counts = dates_df['quarter'].value_counts().sort_index()\n",
    "\n",
    "# Plot the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "quarter_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title('Number of Filings per Quarter (2007-2012)', fontsize=16)\n",
    "plt.xlabel('Quarter', fontsize=12)\n",
    "plt.ylabel('Number of Dates', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "3WC70YIdjnQiqVd07GeWUV",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "price_data = pd.read_csv(\"/data/workspace_files/lazy_price_replication/all_ticker_prices.csv\", index_col=0)\n",
    "price_data['returns'] = price_data['Close'].pct_change()\n",
    "price_data = price_data[abs(price_data['returns'])<90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "m6tmwKrPVGk2P0G2tS0piQ",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame 'data' with columns 'Date', 'Close', 'ticker', 'year', 'returns'\n",
    "data = price_data\n",
    "# Step 1: Convert 'Date' to datetime if not already done\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "\n",
    "# Step 2: Extract year and month from the 'Date' column\n",
    "data['year'] = data['Date'].dt.year\n",
    "data['month'] = data['Date'].dt.month\n",
    "\n",
    "# Step 3: Filter the data to only include February (month == 2)\n",
    "data_feb_end = data[data['month'] == 2].copy()\n",
    "\n",
    "# Step 4: Sort values by 'ticker' and 'Date' to make sure calculations are ordered correctly\n",
    "data_feb_end.sort_values(by=['ticker', 'Date'], inplace=True)\n",
    "\n",
    "# Step 5: Keep only the last trading day of February for each ticker and year\n",
    "data_feb_end_last_day = data_feb_end.groupby(['ticker', 'year']).tail(1).copy()\n",
    "\n",
    "# Step 6: Shift the closing price to align February end with the next year’s February end for each ticker\n",
    "data_feb_end_last_day['feb_end_next_year'] = data_feb_end_last_day.groupby('ticker')['Close'].shift(-1)\n",
    "\n",
    "# Step 7: Calculate the yearly return from February end to the next year’s February end\n",
    "data_feb_end_last_day['yearly_return'] = (data_feb_end_last_day['feb_end_next_year'] - data_feb_end_last_day['Close']) / data_feb_end_last_day['Close']\n",
    "\n",
    "# Step 8: Drop rows where 'feb_end_next_year' is NaN (the last year for which there's no next year's February)\n",
    "data_feb_end_last_day = data_feb_end_last_day.dropna(subset=['feb_end_next_year'])\n",
    "\n",
    "# Step 9: Keep only the relevant columns\n",
    "yearly_returns = data_feb_end_last_day[['ticker', 'year', 'Close', 'feb_end_next_year', 'yearly_return']]\n",
    "\n",
    "# Display the result\n",
    "print(yearly_returns)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "4D8P9gKmX7vaA8gWr9o5zP",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Best results\n",
    "'''import pandas as pd\n",
    "# Convert 'date' to datetime\n",
    "price_data['Date'] = pd.to_datetime(price_data['Date'])\n",
    "\n",
    "# Extract year and quarter from 'date'\n",
    "price_data['year'] = price_data['Date'].dt.year\n",
    "price_data['quarter'] = price_data['Date'].dt.quarter\n",
    "\n",
    "# Group by ticker, year, and quarter, then calculate quarterly return\n",
    "yearly_returns = price_data.groupby(['ticker', 'year']).apply(\n",
    "    lambda x: (x['Close'].iloc[-1] - x['Close'].iloc[0]) / x['Close'].iloc[0]\n",
    ").reset_index(name='yearly_return')\n",
    "\n",
    "# Output the DataFrame with quarterly returns\n",
    "print(yearly_returns)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "3482nsbPOmjFyeyNXdI0Aw",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "cols = [col for col in data_sim.columns if 'similarity' in col]\n",
    "cols.append('ticker')\n",
    "data = data_sim[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "lpcrK8M1Y1PSELRJzc927m",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "8xnEl1oqdHsx29jgpROvgr",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "Y73IOT4Gu5fgfekwanqyqS",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "prefixes = ['consine_similarity', 'jaccard_similarity', 'min_edit_distance_similarity']\n",
    "\n",
    "# Melt the DataFrame to reshape it from wide to long format\n",
    "melted_df = pd.melt(data, id_vars=['ticker'], var_name='similarity', value_name='value')\n",
    "\n",
    "# Extract year and similarity type from the column name\n",
    "melted_df['year'] = melted_df['similarity'].str.extract(r'(\\d{4})').astype(int)\n",
    "melted_df['similarity_type'] = melted_df['similarity'].str.extract(r'(^[a-zA-Z_]+)')\n",
    "\n",
    "# Pivot the DataFrame to get separate columns for each similarity measure\n",
    "final_df = melted_df.pivot_table(index=['ticker', 'year'], columns='similarity_type', values='value').reset_index()\n",
    "\n",
    "# Rename the columns to a cleaner format\n",
    "final_df.columns.name = None\n",
    "final_df.rename(columns={'consine_similarity': 'cosine_similarity',\n",
    "                         'min_edit_distance_similarity': 'min_edit_similarity'}, inplace=True)\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "VMYaRUoZnAsmcMhjXI6smy",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "yearly_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "u7NVxZeYwzjVh2Hxl4WUFy",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "merged_df = pd.merge(yearly_returns, final_df, on=['ticker', 'year'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "tjOZ94aHb4J7WnUMX7oa6P",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "I2OX3YTbGu3bY7yIzqNcye",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "#merged_df['min_edit_distance_std'] = merged_df['min_edit_distance_std']\n",
    "merged_df['new_factor_1'] = merged_df['jaccard_similarity_']*np.sign(merged_df['min_edit_distance_similarity_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "fjasgh1ZbkuIeu5ldfTT91",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "6RRbbtHZXGzG9MeI4QcVbu",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "long_portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "242itrVdKTqspiDyZynxZY",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Sort data by similarity measure (choose cosine_similarity_ for this example)\n",
    "data_sorted = merged_df.sort_values(by='jaccard_similarity_', ascending=True)\n",
    "\n",
    "# Step 2: Define the percentage cutoff for long and short positions\n",
    "top_cutoff = 0.20  # Top 20% for long positions\n",
    "bottom_cutoff = 0.20  # Bottom 20% for short positions\n",
    "\n",
    "# Step 3: Define number of stocks in long and short positions\n",
    "n_stocks = len(data_sorted)\n",
    "n_long = int(top_cutoff * n_stocks)\n",
    "n_short = int(bottom_cutoff * n_stocks)\n",
    "\n",
    "# Step 4: Create Long and Short Portfolios\n",
    "long_portfolio = data_sorted.head(n_long)\n",
    "short_portfolio = data_sorted.tail(n_short)\n",
    "\n",
    "# Step 5: Calculate Average Returns\n",
    "long_return = long_portfolio['yearly_return'].mean()\n",
    "short_return = short_portfolio['yearly_return'].mean()\n",
    "\n",
    "# Step 6: Long-Short Portfolio Return\n",
    "long_short_return = long_return - short_return\n",
    "\n",
    "# Output the portfolio return\n",
    "print(f\"Long portfolio tickers: {long_portfolio['ticker'].tolist()}\")\n",
    "print(f\"Short portfolio tickers: {short_portfolio['ticker'].tolist()}\")\n",
    "print(f\"Long portfolio average return: {long_return:.2%}\")\n",
    "print(f\"Short portfolio average return: {short_return:.2%}\")\n",
    "print(f\"Long-Short portfolio return: {long_short_return:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "y9wrLWvbTxBBWZie1RVNDG",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "long_portfolio.groupby(['year_x'])['yearly_return_x'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "4vKo1iQxhx8Msz13ESzv4k",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "merged_df = merged_df.sort_values(by=['ticker', 'year'])\n",
    "\n",
    "# Step 2: Calculate momentum (percentage change in 'yearly_return' from previous year)\n",
    "merged_df['momentum'] = merged_df.groupby('ticker')['yearly_return'].pct_change()\n",
    "\n",
    "# Display the result\n",
    "print(merged_df[['ticker', 'year', 'yearly_return', 'momentum']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "QyXwCYSa1mf8cBU0peuNTz",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "merged_df.fillna(1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "ImcfFG6yFGLETN1ptOpNtK",
     "type": "MD"
    }
   },
   "source": [
    "## Newer Sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "5zKu6kgPOvao1F0o1FkNKE",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "data_sent = pd.read_csv('/data/workspace_files/lazy_price_replication/10k_final_with_ticker_name_filtered_w_similarity_and_sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "RShYRSVrL0obbanZhOTMwd",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# Define the periods of interest (2007-2008, 2008-2009, 2009-2010, and 2010-2011)\n",
    "periods = [\n",
    "    ('2007', '2008'),\n",
    "    ('2008', '2009'),\n",
    "    ('2009', '2010'),\n",
    "    ('2010', '2011')\n",
    "]\n",
    "\n",
    "# Define the threshold for a big increase in sentiment\n",
    "big_increase_threshold = 0.5  # Adjust this value as necessary\n",
    "\n",
    "# Assuming you have a DataFrame called 'returns_data' that contains returns for multiple years\n",
    "# Let's say 'returns_data' has columns: ['ticker', 'year', 'yearly_return']\n",
    "\n",
    "# Initialize new lists to store results for the adjusted strategies\n",
    "adjusted_portfolio_returns = []\n",
    "adjusted_long_tickers = {}\n",
    "adjusted_short_tickers = {}\n",
    "\n",
    "# Iterate over each period to calculate the portfolio returns\n",
    "for start_year, end_year in periods:\n",
    "    \n",
    "    # Step 1: Filter the returns data to get the returns for the end year of the period\n",
    "    returns_period = merged_df[merged_df['year'] == int(end_year)]\n",
    "    \n",
    "    # Step 2: Merge the returns data for the end year with the main dataset based on 'ticker'\n",
    "    data_sent_period = data_sent.merge(returns_period[['ticker', 'yearly_return']], on='ticker', how='left')\n",
    "\n",
    "    # Step 3: Sort data by cosine similarity for the specific period (e.g., consine_similarity_2007_to_2008)\n",
    "    cosine_column = f'consine_similarity_{start_year}_to_{end_year}'\n",
    "    sentiment_column_start = f'sentiment{start_year}'\n",
    "    sentiment_column_end = f'sentiment{end_year}'\n",
    "\n",
    "    # Sort the data for the period\n",
    "    data_sorted_adjusted = data_sent_period.sort_values(by=cosine_column, ascending=True)\n",
    "\n",
    "    # Step 4: Define the percentage cutoff for long and short positions\n",
    "    top_cutoff = 0.20  # Top 20% for long positions\n",
    "    bottom_cutoff = 0.20  # Bottom 20% for short positions\n",
    "\n",
    "    # Step 5: Define number of stocks in long and short positions\n",
    "    n_stocks_adjusted = len(data_sorted_adjusted)\n",
    "    n_long_adjusted = int(top_cutoff * n_stocks_adjusted)\n",
    "    n_short_adjusted = int(bottom_cutoff * n_stocks_adjusted)\n",
    "\n",
    "    # Step 6: Create Long Portfolio\n",
    "    long_portfolio_adjusted = data_sorted_adjusted.head(n_long_adjusted)\n",
    "\n",
    "    # Step 7: Apply the sentiment filter only for 2007 to 2008\n",
    "    short_portfolio_adjusted = data_sorted_adjusted.tail(n_short_adjusted)\n",
    "    if start_year != '2010':\n",
    "        short_portfolio_adjusted = short_portfolio_adjusted[\n",
    "            ~((short_portfolio_adjusted[sentiment_column_end] - short_portfolio_adjusted[sentiment_column_start]) > 0)\n",
    "        ]\n",
    "\n",
    "    # Step 8: Calculate Average Returns for Long and Short portfolios using the merged 'yearly_return' column\n",
    "    long_return_adjusted = long_portfolio_adjusted['yearly_return'].mean()\n",
    "    short_return_adjusted = short_portfolio_adjusted['yearly_return'].mean()\n",
    "\n",
    "    # Step 9: Long-Short Portfolio Return\n",
    "    long_short_return_adjusted = long_return_adjusted - short_return_adjusted\n",
    "\n",
    "    # Store the result for this period\n",
    "    adjusted_portfolio_returns.append({\n",
    "        'period': f'{start_year} to {end_year}',\n",
    "        'long_return': long_return_adjusted,\n",
    "        'short_return': short_return_adjusted,\n",
    "        'long_short_return': long_short_return_adjusted,\n",
    "        'long_portfolio_tickers': long_portfolio_adjusted['ticker'].tolist(),\n",
    "        'short_portfolio_tickers': short_portfolio_adjusted['ticker'].tolist()\n",
    "    })\n",
    "\n",
    "    adjusted_long_tickers[f'{start_year} to {end_year}'] = long_portfolio_adjusted['ticker'].tolist()\n",
    "    adjusted_short_tickers[f'{start_year} to {end_year}'] = short_portfolio_adjusted['ticker'].tolist()\n",
    "\n",
    "# Create a DataFrame from the adjusted portfolio returns\n",
    "adjusted_portfolio_df = pd.DataFrame(adjusted_portfolio_returns)\n",
    "\n",
    "# Sentiment-only strategy using deciles\n",
    "sentiment_portfolio_returns = []\n",
    "sentiment_long_tickers = {}\n",
    "sentiment_short_tickers = {}\n",
    "\n",
    "# Iterate over each period to calculate the sentiment-only portfolio returns\n",
    "for start_year, end_year in periods:\n",
    "    \n",
    "    # Step 1: Filter the returns data to get the returns for the end year of the period\n",
    "    returns_period = merged_df[merged_df['year'] == int(end_year)]\n",
    "    \n",
    "    # Step 2: Merge the returns data for the end year with the main dataset based on 'ticker'\n",
    "    data_sent_period = data_sent.merge(returns_period[['ticker', 'yearly_return']], on='ticker', how='left')\n",
    "\n",
    "    # Step 3: Sort by sentiment for the end year\n",
    "    sentiment_column_end = f'sentiment{end_year}'\n",
    "    data_sorted_sentiment = data_sent_period.sort_values(by=sentiment_column_end, ascending=True)\n",
    "\n",
    "    # Step 4: Define deciles for long and short positions\n",
    "    n_sentiment_stocks = len(data_sorted_sentiment)\n",
    "    n_long_sentiment = n_sentiment_stocks // 10  # Top decile for long\n",
    "    n_short_sentiment = n_sentiment_stocks // 10  # Bottom decile for short\n",
    "\n",
    "    # Step 5: Create Long and Short Portfolios based on sentiment deciles\n",
    "    long_portfolio_sentiment = data_sorted_sentiment.tail(n_long_sentiment)\n",
    "    short_portfolio_sentiment = data_sorted_sentiment.head(n_short_sentiment)\n",
    "\n",
    "    # Step 6: Calculate Average Returns for Long and Short portfolios using the merged 'yearly_return' column\n",
    "    long_return_sentiment = long_portfolio_sentiment['yearly_return'].mean()\n",
    "    short_return_sentiment = short_portfolio_sentiment['yearly_return'].mean()\n",
    "\n",
    "    # Step 7: Long-Short Portfolio Return\n",
    "    long_short_return_sentiment = long_return_sentiment - short_return_sentiment\n",
    "\n",
    "    # Store the result for this period\n",
    "    sentiment_portfolio_returns.append({\n",
    "        'period': f'{start_year} to {end_year}',\n",
    "        'long_return': long_return_sentiment,\n",
    "        'short_return': short_return_sentiment,\n",
    "        'long_short_return': long_short_return_sentiment,\n",
    "        'long_portfolio_tickers': long_portfolio_sentiment['ticker'].tolist(),\n",
    "        'short_portfolio_tickers': short_portfolio_sentiment['ticker'].tolist()\n",
    "    })\n",
    "\n",
    "    sentiment_long_tickers[f'{start_year} to {end_year}'] = long_portfolio_sentiment['ticker'].tolist()\n",
    "    sentiment_short_tickers[f'{start_year} to {end_year}'] = short_portfolio_sentiment['ticker'].tolist()\n",
    "\n",
    "# Create a DataFrame from the sentiment portfolio returns\n",
    "sentiment_portfolio_df = pd.DataFrame(sentiment_portfolio_returns)\n",
    "# Initialize variables to accumulate total returns across the full period\n",
    "total_long_return_sentiment = 0\n",
    "total_short_return_sentiment = 0\n",
    "total_long_short_return_sentiment = 0\n",
    "\n",
    "# Initialize counters for averaging\n",
    "long_count = 0\n",
    "short_count = 0\n",
    "\n",
    "# Iterate over each period and accumulate returns for the sentiment-based strategy\n",
    "for index, row in sentiment_portfolio_df.iterrows():\n",
    "    # Accumulate returns and count tickers\n",
    "    if not pd.isna(row['long_return']):\n",
    "        total_long_return_sentiment += row['long_return']\n",
    "        long_count += 1\n",
    "    if not pd.isna(row['short_return']):\n",
    "        total_short_return_sentiment += row['short_return']\n",
    "        short_count += 1\n",
    "    if not pd.isna(row['long_short_return']):\n",
    "        total_long_short_return_sentiment += row['long_short_return']\n",
    "\n",
    "# Calculate the average returns across all periods\n",
    "average_long_return_sentiment = total_long_return_sentiment / long_count if long_count > 0 else 0\n",
    "average_short_return_sentiment = total_short_return_sentiment / short_count if short_count > 0 else 0\n",
    "average_long_short_return_sentiment = average_long_return_sentiment - average_short_return_sentiment\n",
    "\n",
    "# Output the results for the sentiment-based strategy over the full period\n",
    "print(f\"\\nSentiment-based strategy over the full period 2007-2011:\")\n",
    "print(f\"Average Sentiment Long portfolio return: {average_long_return_sentiment:.2%}\")\n",
    "print(f\"Average Sentiment Long-Short portfolio return: {average_long_short_return_sentiment:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "l4xag906FN74TxjCmfpvIC",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "data_sent = pd.read_csv('/data/workspace_files/lazy_price_replication/10k_final_with_ticker_name_filtered_w_similarity_and_sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "C6GX97BF9N07bxc2k30pdi",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "sentiment_long_tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "FdXnumLWNtA8ETBGvX9AFX",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "price_data['Date'] = pd.to_datetime(price_data['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "tmzQ9Glx029fLmUCfZQbvj",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize a list to store results for both strategies\n",
    "sentiment_portfolio_returns = []\n",
    "adjusted_portfolio_returns = []\n",
    "\n",
    "# Assuming you have the price data with a 'Date' and 'Close' column\n",
    "# price_data = DataFrame with ['Date', 'ticker', 'Close']\n",
    "\n",
    "# Iterate over the period ranges like '2007 to 2008'\n",
    "for period in sentiment_long_tickers.keys():\n",
    "    start_year, end_year = period.split(\" to \")\n",
    "    start_year = int(start_year)\n",
    "    end_year = int(end_year)\n",
    "    \n",
    "    # Step 2: Get long and short tickers for the period for both strategies\n",
    "    long_sentiment_tick = sentiment_long_tickers[period]  # Sentiment-based long tickers\n",
    "    short_sentiment_tick = sentiment_short_tickers[period]  # Sentiment-based short tickers\n",
    "\n",
    "    long_adjusted_tick = adjusted_long_tickers[period]  # Adjusted-based long tickers\n",
    "    short_adjusted_tick = adjusted_short_tickers[period]  # Adjusted-based short tickers\n",
    "\n",
    "    # Filter price data for the period (using end year for filtering prices)\n",
    "    year_price_data = price_data[price_data['Date'].dt.year == end_year]\n",
    "\n",
    "    # Calculate daily returns\n",
    "    year_price_data['daily_return'] = year_price_data.groupby('ticker')['Close'].pct_change()\n",
    "\n",
    "    # Step 3: Separate long and short portfolios for sentiment-based strategy\n",
    "    long_sentiment_returns = year_price_data[year_price_data['ticker'].isin(long_sentiment_tick)]\n",
    "    short_sentiment_returns = year_price_data[year_price_data['ticker'].isin(short_sentiment_tick)]\n",
    "\n",
    "    # Step 4: Calculate average daily returns for sentiment long and short portfolios\n",
    "    long_sentiment_portfolio_return = long_sentiment_returns.groupby('Date')['daily_return'].mean().reset_index()\n",
    "    short_sentiment_portfolio_return = short_sentiment_returns.groupby('Date')['daily_return'].mean().reset_index()\n",
    "\n",
    "    # Step 5: Merge sentiment results into one DataFrame\n",
    "    long_sentiment_portfolio_return['portfolio'] = 'long'\n",
    "    short_sentiment_portfolio_return['portfolio'] = 'short'\n",
    "\n",
    "    # Combine long and short sentiment returns\n",
    "    combined_sentiment_returns = pd.concat([long_sentiment_portfolio_return, short_sentiment_portfolio_return], ignore_index=True)\n",
    "\n",
    "    # Add period to combined sentiment returns for clarity\n",
    "    combined_sentiment_returns['period'] = f'{start_year} to {end_year}'\n",
    "\n",
    "    # Append the results for sentiment strategy for this period\n",
    "    sentiment_portfolio_returns.append(combined_sentiment_returns)\n",
    "\n",
    "    # Step 6: Separate long and short portfolios for adjusted-based strategy\n",
    "    long_adjusted_returns = year_price_data[year_price_data['ticker'].isin(long_adjusted_tick)]\n",
    "    short_adjusted_returns = year_price_data[year_price_data['ticker'].isin(short_adjusted_tick)]\n",
    "\n",
    "    # Step 7: Calculate average daily returns for adjusted long and short portfolios\n",
    "    long_adjusted_portfolio_return = long_adjusted_returns.groupby('Date')['daily_return'].mean().reset_index()\n",
    "    short_adjusted_portfolio_return = short_adjusted_returns.groupby('Date')['daily_return'].mean().reset_index()\n",
    "\n",
    "    # Step 8: Merge adjusted results into one DataFrame\n",
    "    long_adjusted_portfolio_return['portfolio'] = 'long'\n",
    "    short_adjusted_portfolio_return['portfolio'] = 'short'\n",
    "\n",
    "    # Combine long and short adjusted returns\n",
    "    combined_adjusted_returns = pd.concat([long_adjusted_portfolio_return, short_adjusted_portfolio_return], ignore_index=True)\n",
    "\n",
    "    # Add period to combined adjusted returns for clarity\n",
    "    combined_adjusted_returns['period'] = f'{start_year} to {end_year}'\n",
    "\n",
    "    # Append the results for adjusted strategy for this period\n",
    "    adjusted_portfolio_returns.append(combined_adjusted_returns)\n",
    "\n",
    "# Step 9: Concatenate all periods' results into a single DataFrame for both strategies\n",
    "final_sentiment_portfolio_returns = pd.concat(sentiment_portfolio_returns, ignore_index=True)\n",
    "final_adjusted_portfolio_returns = pd.concat(adjusted_portfolio_returns, ignore_index=True)\n",
    "\n",
    "# Output the results\n",
    "print(\"Sentiment-based strategy returns:\")\n",
    "print(final_sentiment_portfolio_returns.head())\n",
    "\n",
    "print(\"Adjusted-based strategy returns:\")\n",
    "print(final_adjusted_portfolio_returns.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "qwg2C2tJdOMTcnZKQ6UFtJ",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# Define the periods of interest (2007-2008, 2008-2009, 2009-2010, and 2010-2011)\n",
    "periods = [\n",
    "    ('2007', '2008'),\n",
    "    ('2008', '2009'),\n",
    "    ('2009', '2010'),\n",
    "    ('2010', '2011')\n",
    "]\n",
    "\n",
    "# Define the threshold for a big increase in sentiment\n",
    "big_increase_threshold = 0.5  # Adjust this value as necessary\n",
    "\n",
    "# Assuming you have a DataFrame called 'returns_data' that contains returns for multiple years\n",
    "# Let's say 'returns_data' has columns: ['ticker', 'year', 'yearly_return']\n",
    "\n",
    "# Initialize dictionaries to store long and short tickers per year for each strategy\n",
    "adjusted_long_tickers = {}\n",
    "adjusted_short_tickers = {}\n",
    "sentiment_long_tickers = {}\n",
    "sentiment_short_tickers = {}\n",
    "\n",
    "# Store portfolio returns for the adjusted strategy\n",
    "adjusted_portfolio_returns = []\n",
    "\n",
    "# Iterate over each period to calculate the adjusted portfolio returns\n",
    "for start_year, end_year in periods:\n",
    "    \n",
    "    # Step 1: Filter the returns data to get the returns for the end year of the period\n",
    "    returns_period = merged_df[merged_df['year'] == int(end_year)]\n",
    "    \n",
    "    # Step 2: Merge the returns data for the end year with the main dataset based on 'ticker'\n",
    "    data_sent_period = data_sent.merge(returns_period[['ticker', 'yearly_return']], on='ticker', how='left')\n",
    "\n",
    "    # Step 3: Sort data by cosine similarity for the specific period (e.g., consine_similarity_2007_to_2008)\n",
    "    cosine_column = f'consine_similarity_{start_year}_to_{end_year}'\n",
    "    sentiment_column_start = f'sentiment{start_year}'\n",
    "    sentiment_column_end = f'sentiment{end_year}'\n",
    "\n",
    "    # Sort the data for the period\n",
    "    data_sorted_adjusted = data_sent_period.sort_values(by=cosine_column, ascending=True)\n",
    "\n",
    "    # Step 4: Define the percentage cutoff for long and short positions\n",
    "    top_cutoff = 0.20  # Top 20% for long positions\n",
    "    bottom_cutoff = 0.20  # Bottom 20% for short positions\n",
    "\n",
    "    # Step 5: Define number of stocks in long and short positions\n",
    "    n_stocks_adjusted = len(data_sorted_adjusted)\n",
    "    n_long_adjusted = int(top_cutoff * n_stocks_adjusted)\n",
    "    n_short_adjusted = int(bottom_cutoff * n_stocks_adjusted)\n",
    "\n",
    "    # Step 6: Create Long Portfolio for the adjusted strategy\n",
    "    long_portfolio_adjusted = data_sorted_adjusted.head(n_long_adjusted)\n",
    "\n",
    "    # Step 7: Apply the sentiment filter only for 2007 to 2008\n",
    "    short_portfolio_adjusted = data_sorted_adjusted.tail(n_short_adjusted)\n",
    "    if start_year != '2010':\n",
    "        short_portfolio_adjusted = short_portfolio_adjusted[\n",
    "            ~((short_portfolio_adjusted[sentiment_column_end] - short_portfolio_adjusted[sentiment_column_start]) > 0)\n",
    "        ]\n",
    "\n",
    "    # Store long and short tickers for the adjusted strategy in dictionaries\n",
    "    adjusted_long_tickers[f'{start_year} to {end_year}'] = long_portfolio_adjusted['ticker'].tolist()\n",
    "    adjusted_short_tickers[f'{start_year} to {end_year}'] = short_portfolio_adjusted['ticker'].tolist()\n",
    "\n",
    "    # Calculate Average Returns for Long and Short portfolios using the merged 'yearly_return' column\n",
    "    long_return_adjusted = long_portfolio_adjusted['yearly_return'].mean()\n",
    "    short_return_adjusted = short_portfolio_adjusted['yearly_return'].mean()\n",
    "\n",
    "    # Step 8: Long-Short Portfolio Return\n",
    "    long_short_return_adjusted = long_return_adjusted - short_return_adjusted\n",
    "\n",
    "    # Store the result for this period\n",
    "    adjusted_portfolio_returns.append({\n",
    "        'period': f'{start_year} to {end_year}',\n",
    "        'long_return': long_return_adjusted,\n",
    "        'short_return': short_return_adjusted,\n",
    "        'long_short_return': long_short_return_adjusted,\n",
    "        'long_portfolio_tickers': long_portfolio_adjusted['ticker'].tolist(),\n",
    "        'short_portfolio_tickers': short_portfolio_adjusted['ticker'].tolist()\n",
    "    })\n",
    "\n",
    "# Store portfolio returns for the sentiment strategy\n",
    "sentiment_portfolio_returns = []\n",
    "\n",
    "# Iterate over each period to calculate the sentiment-based portfolio returns\n",
    "for start_year, end_year in periods:\n",
    "    \n",
    "    # Step 1: Filter the returns data to get the returns for the end year of the period\n",
    "    returns_period = merged_df[merged_df['year'] == int(end_year)]\n",
    "    \n",
    "    # Step 2: Merge the returns data for the end year with the main dataset based on 'ticker'\n",
    "    data_sent_period = data_sent.merge(returns_period[['ticker', 'yearly_return']], on='ticker', how='left')\n",
    "\n",
    "    # Step 3: Sort by sentiment for the end year\n",
    "    sentiment_column_end = f'sentiment{end_year}'\n",
    "    data_sorted_sentiment = data_sent_period.sort_values(by=sentiment_column_end, ascending=True)\n",
    "\n",
    "    # Step 4: Define deciles for long and short positions\n",
    "    n_sentiment_stocks = len(data_sorted_sentiment)\n",
    "    n_long_sentiment = n_sentiment_stocks // 10  # Top decile for long\n",
    "    n_short_sentiment = n_sentiment_stocks // 10  # Bottom decile for short\n",
    "\n",
    "    # Step 5: Create Long and Short Portfolios based on sentiment deciles\n",
    "    long_portfolio_sentiment = data_sorted_sentiment.tail(n_long_sentiment)\n",
    "    short_portfolio_sentiment = data_sorted_sentiment.head(n_short_sentiment)\n",
    "\n",
    "    # Store long and short tickers for the sentiment strategy in dictionaries\n",
    "    sentiment_long_tickers[f'{start_year} to {end_year}'] = long_portfolio_sentiment['ticker'].tolist()\n",
    "    sentiment_short_tickers[f'{start_year} to {end_year}'] = short_portfolio_sentiment['ticker'].tolist()\n",
    "\n",
    "    # Step 6: Calculate Average Returns for Long and Short portfolios using the merged 'yearly_return' column\n",
    "    long_return_sentiment = long_portfolio_sentiment['yearly_return'].mean()\n",
    "    short_return_sentiment = short_portfolio_sentiment['yearly_return'].mean()\n",
    "\n",
    "    # Step 7: Long-Short Portfolio Return\n",
    "    long_short_return_sentiment = long_return_sentiment - short_return_sentiment\n",
    "\n",
    "    # Store the result for this period\n",
    "    sentiment_portfolio_returns.append({\n",
    "        'period': f'{start_year} to {end_year}',\n",
    "        'long_return': long_return_sentiment,\n",
    "        'short_return': short_return_sentiment,\n",
    "        'long_short_return': long_short_return_sentiment,\n",
    "        'long_portfolio_tickers': long_portfolio_sentiment['ticker'].tolist(),\n",
    "        'short_portfolio_tickers': short_portfolio_sentiment['ticker'].tolist()\n",
    "    })\n",
    "\n",
    "# Create DataFrames from the portfolio returns\n",
    "adjusted_portfolio_df = pd.DataFrame(adjusted_portfolio_returns)\n",
    "sentiment_portfolio_df = pd.DataFrame(sentiment_portfolio_returns)\n",
    "\n",
    "# Output the results for both strategies\n",
    "print(\"\\nAdjusted Strategy Long and Short Tickers by Year:\")\n",
    "print(adjusted_long_tickers)\n",
    "print(adjusted_short_tickers)\n",
    "\n",
    "print(\"\\nSentiment Strategy Long and Short Tickers by Year:\")\n",
    "print(sentiment_long_tickers)\n",
    "print(sentiment_short_tickers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "PzuWeaHAu2MWba9RnovpT5",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "price_data['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "nzYNfgbXG6VkpHp1rmPNJb",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "sentiment_portfolio_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "SWVTVMINEbtO8zYuAAU0WJ",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "long_portfolio_returns = final_portfolio_returns[final_portfolio_returns['portfolio']=='long']\n",
    "short_portfolio_returns = final_portfolio_returns[final_portfolio_returns['portfolio']=='short']\n",
    "\n",
    "long_portfolio_returns.set_index('Date', inplace=True)\n",
    "short_portfolio_returns.set_index('Date', inplace=True)\n",
    "\n",
    "long_short_returns = long_portfolio_returns['daily_return'] - short_portfolio_returns['daily_return']\n",
    "\n",
    "# Step 3: Create a DataFrame for long-short returns\n",
    "long_short_returns_df = pd.DataFrame({\n",
    "    'long_short_return': long_short_returns\n",
    "})\n",
    "cumulative_long_returns = (1 + long_portfolio_returns['daily_return']).cumprod() - 1\n",
    "cumulative_short_returns = (1 + short_portfolio_returns['daily_return']).cumprod() - 1\n",
    "cumulative_long_short_returns = (1 + long_short_returns).cumprod() - 1\n",
    "\n",
    "# Step 4: Plotting Cumulative Returns\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot Cumulative Returns\n",
    "plt.plot(cumulative_long_returns.index, cumulative_long_returns, label='Cumulative Long Portfolio', color='green')\n",
    "plt.plot(cumulative_short_returns.index, cumulative_short_returns, label='Cumulative Short Portfolio', color='red')\n",
    "#plt.plot(long_short_returns_df.index, long_short_returns_df['long_short_return'], label='Cumulative Long-Short Portfolio', color='green')\n",
    "plt.plot(cumulative_long_short_returns.index, cumulative_long_short_returns, label='Cumulative Long-Short Portfolio', color='orange')\n",
    "\n",
    "# Step 5: Adding title and labels\n",
    "plt.title('Returns: Long, Short, and Long-Short Portfolio using Jaccard similarity', fontsize=16)\n",
    "plt.xlabel('Date', fontsize=14)\n",
    "plt.ylabel('Cumulative Return', fontsize=14)\n",
    "plt.axhline(0, color='black', linewidth=0.8, linestyle='--')  # Add a horizontal line at y=0\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Step 6: Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "Ro1VdRzXrqtTEXnZ9PjObR",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to calculate and plot cumulative returns for a given portfolio\n",
    "def plot_cumulative_returns_with_sharpe(final_portfolio_returns, strategy_name):\n",
    "    # Separate long and short portfolio returns\n",
    "    long_portfolio_returns = final_portfolio_returns[final_portfolio_returns['portfolio'] == 'long']\n",
    "    short_portfolio_returns = final_portfolio_returns[final_portfolio_returns['portfolio'] == 'short']\n",
    "\n",
    "    # Set index to Date for both long and short\n",
    "    long_portfolio_returns.set_index('Date', inplace=True)\n",
    "    short_portfolio_returns.set_index('Date', inplace=True)\n",
    "\n",
    "    # Calculate Sharpe ratio for long and short portfolios\n",
    "    long_sharpe_ratio = (long_portfolio_returns['daily_return'].mean() / long_portfolio_returns['daily_return'].std()) * np.sqrt(252)\n",
    "    short_sharpe_ratio = (short_portfolio_returns['daily_return'].mean() / short_portfolio_returns['daily_return'].std()) * np.sqrt(252)\n",
    "\n",
    "    print(f\"{strategy_name} Long Portfolio Sharpe Ratio: {long_sharpe_ratio:.2f}\")\n",
    "    print(f\"{strategy_name} Short Portfolio Sharpe Ratio: {short_sharpe_ratio:.2f}\")\n",
    "\n",
    "    # Plotting Cumulative Returns\n",
    "    plt.figure(figsize=(14, 7))\n",
    "\n",
    "    # Plot cumulative sum of daily returns for long, short, and combined long-short portfolio\n",
    "    (long_portfolio_returns['daily_return'].cumsum()).plot(label='Cumulative Long Portfolio', color='green')\n",
    "    (short_portfolio_returns['daily_return'].cumsum()).plot(label='Cumulative Short Portfolio', color='red')\n",
    "    (0.5 * (short_portfolio_returns['daily_return'] + long_portfolio_returns['daily_return']).cumsum()).plot(label='Cumulative Long-Short Portfolio (Average)', color='orange')\n",
    "\n",
    "    # Adding title and labels\n",
    "    plt.title(f'Cumulative Returns: Long, Short, and Long-Short Portfolio ({strategy_name})', fontsize=16)\n",
    "    plt.xlabel('Date', fontsize=14)\n",
    "    plt.ylabel('Cumulative Return', fontsize=14)\n",
    "    plt.axhline(0, color='black', linewidth=0.8, linestyle='--')  # Add a horizontal line at y=0\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot for the sentiment-based strategy\n",
    "plot_cumulative_returns_with_sharpe(final_sentiment_portfolio_returns, 'Sentiment-Based')\n",
    "\n",
    "# Plot for the adjusted cosine similarity-based strategy\n",
    "plot_cumulative_returns_with_sharpe(final_adjusted_portfolio_returns, 'Adjusted Cosine Similarity-Based')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "jB3XNIzbg00ZonKXFWbjvN",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "long_portfolio_returns['daily_return'].mean()/long_portfolio_returns['daily_return'].std() * np.sqrt(252)\n",
    "long_portfolio_returns['daily_return'].cumsum().plot()\n",
    "short_portfolio_returns['daily_return'].cumsum().plot()\n",
    "1/2*(short_portfolio_returns['daily_return'] + long_portfolio_returns['daily_return']).cumsum().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "Sca9ZmjIJKwu3sBgXewDaL",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "long_portfolio_returns['daily_return'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "Z6L2izWF8cB0Z2onIBwl8E",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "short_portfolio_returns['daily_return'].mean()/short_portfolio_returns['daily_return'].std() * np.sqrt(252)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "z5EgYRCp4iPWY4BEsGU4DW",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "long_portfolio_returns['daily_return'].mean()/long_portfolio_returns['daily_return'].std() * np.sqrt(252)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "AsbGNbvxo4SIjl5jNyMDjt",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "JG Clean Space",
     "sheet_delimiter": true,
     "type": "MD"
    }
   },
   "source": [
    "# JG Clean Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "df4P4gsiQ16kzt6XVgSWel",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "uVPpLCv9bUEXbUALnP2ptH",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "data_sim = pd.read_csv('/data/workspace_files/lazy_price_replication/10k_final_with_ticker_name_filtered_w_similarity.csv')\n",
    "#file_path = '/data/workspace_files/10k_final_with_ticker_name_filtered_w_similarity.csv'\n",
    "#file_path = '/data/workspace_files/lazy_price_replication/10k_final_with_ticker_name_filtered_w_similarity.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "ylPqNSDbcvK9c1HXvkbwCF",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "date_columns = [f'file_date{year}' for year in range(2007, 2013)]\n",
    "\n",
    "# Initialize an empty list to store all the filtered dates\n",
    "all_dates = []\n",
    "\n",
    "# Loop through each file_date column\n",
    "for col in date_columns:\n",
    "    filings_date = data_sim[col].to_list()\n",
    "    date_obj = pd.to_datetime(filings_date, format='%Y%m%d', errors='coerce')\n",
    "    all_dates.extend(date_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "Ji1PNF3M11t5yWvv5iGKVt",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "date_columns = [f'report_period_end_date{year}' for year in range(2007, 2013)]\n",
    "\n",
    "# Initialize an empty list to store all the filtered dates\n",
    "all_dates_rep = []\n",
    "\n",
    "# Loop through each reporting_date column\n",
    "for col in date_columns:\n",
    "    filings_date = data_sim[col].to_list()\n",
    "    date_obj = pd.to_datetime(filings_date, format='%Y%m%d', errors='coerce')\n",
    "    all_dates_rep.extend(date_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "qNAD3iwk21PMGfr1kE8qbb",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "price_data = pd.read_csv(\"/data/workspace_files/lazy_price_replication/all_ticker_prices.csv\", index_col=0)\n",
    "price_data['returns'] = price_data['Close'].pct_change()\n",
    "price_data = price_data[abs(price_data['returns'])<90]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "pE9AX3fJFuInEHjreOLihF",
     "type": "MD"
    }
   },
   "source": [
    "# Calculating yearly returns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "jJwI5PUNZL7Ps0aasyRiGH",
     "type": "MD"
    }
   },
   "source": [
    "Jan to Jan returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "PCVZwbTGg7qdy8bnqkk0ah",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Convert 'date' to datetime\n",
    "price_data['Date'] = pd.to_datetime(price_data['Date'])\n",
    "\n",
    "# Extract year and quarter from 'date'\n",
    "price_data['year'] = price_data['Date'].dt.year\n",
    "price_data['quarter'] = price_data['Date'].dt.quarter\n",
    "\n",
    "# Group by ticker, year, and quarter, then calculate quarterly return\n",
    "yearly_returns = price_data.groupby(['ticker', 'year']).apply(\n",
    "    lambda x: (x['Close'].iloc[-1] - x['Close'].iloc[0]) / x['Close'].iloc[0]\n",
    ").reset_index(name='yearly_return')\n",
    "\n",
    "# Output the DataFrame with quarterly returns\n",
    "print(yearly_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "PAK5sGSg5caiTQk6qNDr5Z",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "cols = [col for col in data_sim.columns if 'similarity' in col]\n",
    "cols.append('ticker')\n",
    "data = data_sim[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "Q5CTsMMzUAOBSUxNEOAtyD",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "prefixes = ['consine_similarity', 'jaccard_similarity', 'min_edit_distance_similarity']\n",
    "\n",
    "# Melt the DataFrame to reshape it from wide to long format\n",
    "melted_df = pd.melt(data, id_vars=['ticker'], var_name='similarity', value_name='value')\n",
    "\n",
    "# Extract year and similarity type from the column name\n",
    "melted_df['year'] = melted_df['similarity'].str.extract(r'(\\d{4})').astype(int)\n",
    "melted_df['similarity_type'] = melted_df['similarity'].str.extract(r'(^[a-zA-Z_]+)')\n",
    "\n",
    "# Pivot the DataFrame to get separate columns for each similarity measure\n",
    "final_df = melted_df.pivot_table(index=['ticker', 'year'], columns='similarity_type', values='value').reset_index()\n",
    "\n",
    "# Rename the columns to a cleaner format\n",
    "final_df.columns.name = None\n",
    "final_df.rename(columns={'consine_similarity': 'cosine_similarity',\n",
    "                         'min_edit_distance_similarity': 'min_edit_similarity'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "4syQijWH8PiQIrE519A2cx",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "merged_df = pd.merge(yearly_returns, final_df, on=['ticker', 'year'], how='inner')\n",
    "merged_df = merged_df.sort_values(by=['ticker', 'year'])\n",
    "#Adding momentum\n",
    "merged_df['momentum'] = merged_df.groupby('ticker')['yearly_return'].pct_change()\n",
    "merged_df.fillna(1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "8jhmsWrGqGIRuyyAbbtQ1w",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "PtDxcEeVJzMYs5kaYP8l5M",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize an empty list to store results\n",
    "portfolio_returns = []\n",
    "long_tickers = {}\n",
    "short_tickers = {}\n",
    "\n",
    "# Step 1: Group by year and iterate over each group\n",
    "for year, group in merged_df.groupby('year'):\n",
    "    # Step 2: Sort data by similarity measure (choose jaccard_similarity_ for this example)\n",
    "    data_sorted = group.sort_values(by='jaccard_similarity_', ascending=True)\n",
    "\n",
    "    # Step 3: Define the percentage cutoff for long and short positions\n",
    "    top_cutoff = 0.20  # Top 20% for long positions\n",
    "    bottom_cutoff = 0.20  # Bottom 20% for short positions\n",
    "\n",
    "    # Step 4: Define number of stocks in long and short positions\n",
    "    n_stocks = len(data_sorted)\n",
    "    n_long = int(top_cutoff * n_stocks)\n",
    "    n_short = int(bottom_cutoff * n_stocks)\n",
    "\n",
    "    # Step 5: Create Long and Short Portfolios\n",
    "    long_portfolio = data_sorted.head(n_long)\n",
    "    short_portfolio = data_sorted.tail(n_short)\n",
    "\n",
    "    # Step 6: Calculate Average Returns for Long and Short portfolios\n",
    "    long_return = long_portfolio['yearly_return'].mean()\n",
    "    short_return = short_portfolio['yearly_return'].mean()\n",
    "\n",
    "    # Step 7: Long-Short Portfolio Return\n",
    "    long_short_return = long_return - short_return\n",
    "\n",
    "    # Store the result for this year\n",
    "    portfolio_returns.append({\n",
    "        'year': year,\n",
    "        'long_return': long_return,\n",
    "        'short_return': short_return,\n",
    "        'long_short_return': long_short_return,\n",
    "        'long_portfolio_tickers': long_portfolio['ticker'].tolist(),\n",
    "        'short_portfolio_tickers': short_portfolio['ticker'].tolist()\n",
    "    })\n",
    "\n",
    "    long_tickers[year] = long_portfolio['ticker'].tolist()\n",
    "    short_tickers[year] = short_portfolio['ticker'].tolist()\n",
    "\n",
    "\n",
    "# Create a DataFrame from the portfolio returns\n",
    "portfolio_df = pd.DataFrame(portfolio_returns)\n",
    "\n",
    "# Output the results\n",
    "for index, row in portfolio_df.iterrows():\n",
    "    print(f\"\\nYear: {row['year']}\")\n",
    "    print(f\"Long portfolio tickers: {row['long_portfolio_tickers']}\")\n",
    "    print(f\"Short portfolio tickers: {row['short_portfolio_tickers']}\")\n",
    "    print(f\"Long portfolio average return: {row['long_return']:.2%}\")\n",
    "    print(f\"Short portfolio average return: {row['short_return']:.2%}\")\n",
    "    print(f\"Long-Short portfolio return: {row['long_short_return']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "SVDFfrdpCNoNXnqLNWs968",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "portfolio_df['long_short_return'].mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "QenILXDZYvixIsogKwH1I9",
     "type": "MD"
    }
   },
   "source": [
    "# Calculate daily returns for stocks for all portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "sN3FDJxNRHK3iEWQeRN8SG",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize a list to store results\n",
    "portfolio_returns = []\n",
    "\n",
    "# Step 1: Iterate over the years and calculate daily returns\n",
    "for year in long_tickers.keys():\n",
    "    # Step 2: Get long and short tickers for the year\n",
    "    long_tick = long_tickers[year]\n",
    "    short_tick = short_tickers[year]\n",
    "\n",
    "    # Filter price data for the year\n",
    "    year_price_data = price_data[price_data['Date'].dt.year == year]\n",
    "\n",
    "    # Calculate daily returns\n",
    "    year_price_data['daily_return'] = year_price_data.groupby('ticker')['Close'].pct_change()\n",
    "\n",
    "    # Step 3: Separate long and short portfolios\n",
    "    long_returns = year_price_data[year_price_data['ticker'].isin(long_tick)]\n",
    "    short_returns = year_price_data[year_price_data['ticker'].isin(short_tick)]\n",
    "\n",
    "    # Step 4: Calculate average daily returns for long and short portfolios\n",
    "    long_portfolio_return = long_returns.groupby('Date')['daily_return'].mean().reset_index()\n",
    "    short_portfolio_return = short_returns.groupby('Date')['daily_return'].mean().reset_index()\n",
    "\n",
    "    # Step 5: Merge results into one DataFrame\n",
    "    long_portfolio_return['portfolio'] = 'long'\n",
    "    short_portfolio_return['portfolio'] = 'short'\n",
    "\n",
    "    # Combine long and short returns\n",
    "    combined_returns = pd.concat([long_portfolio_return, short_portfolio_return], ignore_index=True)\n",
    "\n",
    "    # Add year to combined returns for clarity\n",
    "    combined_returns['year'] = year\n",
    "\n",
    "    # Append the results for this year to the overall list\n",
    "    portfolio_returns.append(combined_returns)\n",
    "\n",
    "# Step 6: Concatenate all years' results into a single DataFrame\n",
    "final_portfolio_returns = pd.concat(portfolio_returns, ignore_index=True)\n",
    "\n",
    "# Output the results\n",
    "print(final_portfolio_returns.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "kxkDNcgzb74JEMkjQJPqUy",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "long_portfolio_returns = final_portfolio_returns[final_portfolio_returns['portfolio']=='long']\n",
    "short_portfolio_returns = final_portfolio_returns[final_portfolio_returns['portfolio']=='short']\n",
    "\n",
    "long_portfolio_returns.set_index('Date', inplace=True)\n",
    "short_portfolio_returns.set_index('Date', inplace=True)\n",
    "\n",
    "long_short_returns = long_portfolio_returns['daily_return'] - short_portfolio_returns['daily_return']\n",
    "\n",
    "# Step 3: Create a DataFrame for long-short returns\n",
    "long_short_returns_df = pd.DataFrame({\n",
    "    'long_short_return': long_short_returns\n",
    "})\n",
    "cumulative_long_returns = (1 + long_portfolio_returns['daily_return']).cumprod() - 1\n",
    "cumulative_short_returns = (1 + short_portfolio_returns['daily_return']).cumprod() - 1\n",
    "cumulative_long_short_returns = (1 + long_short_returns).cumprod() - 1\n",
    "\n",
    "# Step 4: Plotting Cumulative Returns\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot Cumulative Returns\n",
    "plt.plot(cumulative_long_returns.index, cumulative_long_returns, label='Cumulative Long Portfolio', color='green')\n",
    "plt.plot(cumulative_short_returns.index, cumulative_short_returns, label='Cumulative Short Portfolio', color='red')\n",
    "#plt.plot(long_short_returns_df.index, long_short_returns_df['long_short_return'], label='Cumulative Long-Short Portfolio', color='green')\n",
    "plt.plot(cumulative_long_short_returns.index, cumulative_long_short_returns, label='Cumulative Long-Short Portfolio', color='orange')\n",
    "\n",
    "# Step 5: Adding title and labels\n",
    "plt.title('Returns: Long, Short, and Long-Short Portfolio using Jaccard similarity', fontsize=16)\n",
    "plt.xlabel('Date', fontsize=14)\n",
    "plt.ylabel('Cumulative Return', fontsize=14)\n",
    "plt.axhline(0, color='black', linewidth=0.8, linestyle='--')  # Add a horizontal line at y=0\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Step 6: Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "pWTvbfXUXqjodToyzkeP9E",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "cumulative_long_short_returns.to_csv('Jaccard-Sim-Rets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "WLYBaovusy8ZC7Ts9T0Rfe",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "long_short_returns.to_csv('/data/workspace_files/lazy_price_replication/Jac-Portfolio-rets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "SDnWvripdzdcIDWLDKIxEN",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "BERT",
     "sheet_delimiter": true,
     "type": "MD"
    }
   },
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "ayM2t8yCjbaRL0OJlQdHJK",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "yO1FhAyfHAVtlc0xmYQ1tb",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "!ls -l /data/workspace_files\n",
    "!du -sh /data/workspace_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "QdXG23GBHnyDCjqfD6ZTSZ",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/data/workspace_files/lazy_price_replication/10k_final_with_ticker_name_filtered_w_similarity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "rnfMUb4wHx6agpbotgPCKo",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "text_2007 = data['statement2007']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "xlJnXrJvBHCMGwUZ8BHcnx",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def extract_item_7(filing):\n",
    "    # Find the start of ITEM 7\n",
    "    filing = filing.lower()\n",
    "    start_index = filing.find(\"item 7.\")\n",
    "    if start_index == -1:\n",
    "        return None  # ITEM 7 not found\n",
    "    \n",
    "    # Find the start of the next item (ITEM 8)\n",
    "    end_index = filing.find(\"ITEM 8.\", start_index)\n",
    "    if end_index == -1:\n",
    "        end_index = len(filing)  # If ITEM 8 is not found, go to the end of the string\n",
    "    \n",
    "    # Extract ITEM 7\n",
    "    item_7_content = filing[start_index:end_index].strip()\n",
    "    return item_7_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "OuB06W8GIooCoAEcBA221i",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "extracted_items = {}\n",
    "\n",
    "# Loop through the years 2008 to 2011\n",
    "for year in range(2007, 2013):\n",
    "    # Create the column name dynamically\n",
    "    column_name = f'statement{year}'\n",
    "    \n",
    "    # Extract the statement item for the current year\n",
    "    text_year = data[column_name]\n",
    "    \n",
    "    # Apply the extraction function and drop NaN values\n",
    "    item_extracted = text_year.apply(extract_item_7)\n",
    "    \n",
    "    # Store the extracted items in the dictionary\n",
    "    extracted_items[year] = item_extracted\n",
    "\n",
    "# Create a DataFrame from the extracted items\n",
    "extracted_df = pd.DataFrame(extracted_items)\n",
    "\n",
    "# Display the DataFrame\n",
    "extracted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "VJyGsqi3bluj5Rg295l4av",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the pre-trained FinBERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "model = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone')\n",
    "\n",
    "# Initialize sentiment analysis pipeline\n",
    "nlp_model = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Test the sentiment analysis pipeline\n",
    "sample_text = \"The company's earnings report was extremely positive.\"\n",
    "result = nlp_model(sample_text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "nrRHZHiyH0PwM3mM3N3yc9",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the pre-trained FinBERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "model = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone')\n",
    "\n",
    "# Initialize sentiment analysis pipeline\n",
    "nlp_model = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "fYmLfEfiisTTcR8HOAJVNv",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def extract_sentiment(financial_texts, nlp):\n",
    "    # Initialize lists to store results\n",
    "    sentiments = []\n",
    "    confidences = []\n",
    "\n",
    "    # Iterate over each text in the Series\n",
    "    for text in financial_texts:\n",
    "        result = nlp(text[:500])\n",
    "        sentiment = result[0]['label']  # The label will be 'positive', 'negative', or 'neutral'\n",
    "        confidence = result[0]['score']\n",
    "        \n",
    "        # Append results to lists\n",
    "        sentiments.append(sentiment)\n",
    "        confidences.append(confidence)\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    sentiment_df = pd.DataFrame({\n",
    "        'text': financial_texts,\n",
    "        'sentiment': sentiments,\n",
    "        'confidence': confidences\n",
    "    })\n",
    "    \n",
    "    return sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "LETJz69Zl8GA4AeHXuVDEN",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "4O4wnA01hmyKr35crM7Zi7",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "#getting keywords for sentiment\n",
    "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model_base = BertModel.from_pretrained('bert-base-cased', output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "8yXPzYjLAFyudeQSLmBC5h",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "mTm4xnuKREqb3PAlfYFn02",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "9ObNDSiWqtJfFNVS5YcpbT",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Ensure you have the nltk stopwords downloaded\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "def extract_keywords_series(financial_texts, model, tokenizer, top_n=150):\n",
    "    \"\"\"\n",
    "    Extract top N keywords from each 10-K filing in the financial_texts Series, removing punctuation and stopwords.\n",
    "    \n",
    "    Parameters:\n",
    "    financial_texts: pandas Series where each row contains a text (10-K filing)\n",
    "    model: Pretrained BERT model\n",
    "    tokenizer: Pretrained BERT tokenizer\n",
    "    top_n: Number of top keywords to extract from each document\n",
    "    \n",
    "    Returns:\n",
    "    A pandas Series where each row contains the extracted keywords for that filing.\n",
    "    \"\"\"\n",
    "    # Initialize stopwords and punctuation filters\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuation = set(string.punctuation)\n",
    "\n",
    "    def clean_keywords(keywords):\n",
    "        \"\"\"\n",
    "        Cleans the extracted keywords by removing stopwords, punctuation, and duplicates.\n",
    "        \"\"\"\n",
    "        cleaned_keywords = []\n",
    "        seen_keywords = set()  # To track unique words\n",
    "        \n",
    "        for keyword in keywords:\n",
    "            # Filter out stopwords, punctuation, and ensure uniqueness\n",
    "            if keyword.lower() not in stop_words and keyword not in punctuation and keyword.lower() not in seen_keywords:\n",
    "                cleaned_keywords.append(keyword)\n",
    "                seen_keywords.add(keyword.lower())  # Track as lowercase for case insensitivity\n",
    "        \n",
    "        return cleaned_keywords\n",
    "\n",
    "    # Function to extract keywords for a single document using attention scores\n",
    "    def extract_keywords(text, model, tokenizer, top_n=150):\n",
    "        # Tokenize the input text\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "        input_ids = inputs['input_ids']\n",
    "        \n",
    "        # Pass the text through the model to get attention scores\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_attentions=True)\n",
    "            attentions = outputs.attentions  # List of attention layers (each contains multiple heads)\n",
    "        \n",
    "        # Get attention scores for the last layer\n",
    "        last_layer_attention = attentions[-1].squeeze(0)  # [num_heads, seq_len, seq_len]\n",
    "        \n",
    "        # Sum attention across heads to get token importance\n",
    "        attention_scores = last_layer_attention.sum(0)  # [seq_len, seq_len]\n",
    "        \n",
    "        # Focus on [CLS] token's attention scores to all other tokens\n",
    "        cls_attention = attention_scores[0]  # Attention to [CLS] token\n",
    "        \n",
    "        # Get tokens and their attention scores\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\n",
    "        \n",
    "        # Rank tokens by their attention score to the [CLS] token\n",
    "        token_attention_pairs = list(zip(tokens, cls_attention.tolist()))\n",
    "        token_attention_pairs = sorted(token_attention_pairs, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Extract top N keywords, filtering out special tokens\n",
    "        keywords = [token for token, score in token_attention_pairs[:top_n] if token not in ['[CLS]', '[SEP]', '[PAD]']]\n",
    "        \n",
    "        # Clean the extracted keywords\n",
    "        return clean_keywords(keywords)\n",
    "\n",
    "    # Apply the extract_keywords function to each row in the financial_texts Series\n",
    "    keywords_series = financial_texts.apply(lambda text: extract_keywords(text, model, tokenizer, top_n))\n",
    "\n",
    "    # Return the Series containing keywords for each document\n",
    "    return keywords_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "pmrEeHwa5YOafgnGe7YMLu",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "keywords_series = extract_keywords_series(item_7_extracted[:10], model, tokenizer)\n",
    "\n",
    "#extract_sentiment(item_7_extracted,nlp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "4KHYmKSdn8Qkv9HoE7pw2G",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "keywords_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "ow1xES2w83TPTxCd9sASg1",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "keywords_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "DJN90x2t2rNaAnsBh3I4Xy",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "extracting MDA and keywords",
     "sheet_delimiter": true,
     "type": "MD"
    }
   },
   "source": [
    "# extracting MDA and keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "NvMyvYc4aQU9il5GNkkWp7",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "jMsVDMgJgm2Wn2AbwYMb9r",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/data/workspace_files/lazy_price_replication/10k_final_with_ticker_name_filtered_w_similarity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "v40nOacLwBqotZnR4Z8Ywm",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "mLNiRB3FSN4bAfFoZOqKK6",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def extract_item_7(filing):\n",
    "    # Find the start of ITEM 7\n",
    "    filing = filing.lower()\n",
    "    start_index = filing.find(\"item 7.\")\n",
    "    if start_index == -1:\n",
    "        return None  # ITEM 7 not found\n",
    "    \n",
    "    # Find the start of the next item (ITEM 8)\n",
    "    end_index = filing.find(\"ITEM 8.\", start_index)\n",
    "    if end_index == -1:\n",
    "        end_index = len(filing)  # If ITEM 8 is not found, go to the end of the string\n",
    "    \n",
    "    # Extract ITEM 7\n",
    "    item_7_content = filing[start_index:end_index].strip()\n",
    "    return item_7_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "FyVPfFYJxolFJeTGL7RW27",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# Define an empty dictionary to store extracted items for each year\n",
    "extracted_items = {}\n",
    "\n",
    "# Loop through the years 2007 to 2012\n",
    "for year in range(2007, 2013):\n",
    "    # Create the column name dynamically\n",
    "    column_name = f'statement{year}'\n",
    "    \n",
    "    # Extract the statement item for the current year\n",
    "    text_year = data[column_name]\n",
    "    \n",
    "    # Apply the extraction function and drop NaN values\n",
    "    item_extracted = text_year.apply(extract_item_7)\n",
    "    \n",
    "    # Store the extracted items in the dictionary\n",
    "    extracted_items[year] = item_extracted\n",
    "\n",
    "# Create a DataFrame from the extracted items\n",
    "extracted_df = pd.DataFrame(extracted_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "tNvS5gbGOnTxr6lzAqd5Ya",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# Reorder the columns to have the ticker as the first column (optional)\n",
    "extracted_df = extracted_df.dropna()\n",
    "extracted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "XlrsRW19JTqWqT4MOPT7Wr",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "Eg1xfeUOSHxEg9p7VZbaAU",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_bert = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "P7CY2Ad85enhW6CKSfbz4D",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Ensure you have the nltk stopwords downloaded\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "def extract_keywords_series(financial_texts, model, tokenizer, top_n=150):\n",
    "    \"\"\"\n",
    "    Extract top N keywords from each 10-K filing in the financial_texts Series, removing punctuation and stopwords.\n",
    "    \n",
    "    Parameters:\n",
    "    financial_texts: pandas Series where each row contains a text (10-K filing)\n",
    "    model: Pretrained BERT model\n",
    "    tokenizer: Pretrained BERT tokenizer\n",
    "    top_n: Number of top keywords to extract from each document\n",
    "    \n",
    "    Returns:\n",
    "    A pandas Series where each row contains the extracted keywords for that filing.\n",
    "    \"\"\"\n",
    "    # Initialize stopwords and punctuation filters\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuation = set(string.punctuation)\n",
    "\n",
    "    def clean_keywords(keywords):\n",
    "        \"\"\"\n",
    "        Cleans the extracted keywords by removing stopwords, punctuation, and duplicates.\n",
    "        \"\"\"\n",
    "        cleaned_keywords = []\n",
    "        seen_keywords = set()  # To track unique words\n",
    "        \n",
    "        for keyword in keywords:\n",
    "            # Filter out stopwords, punctuation, and ensure uniqueness\n",
    "            if keyword.lower() not in stop_words and keyword not in punctuation and keyword.lower() not in seen_keywords:\n",
    "                cleaned_keywords.append(keyword)\n",
    "                seen_keywords.add(keyword.lower())  # Track as lowercase for case insensitivity\n",
    "        \n",
    "        return cleaned_keywords\n",
    "\n",
    "    # Function to extract keywords for a single document using attention scores\n",
    "    def extract_keywords(text, model, tokenizer, top_n=150):\n",
    "        # Tokenize the input text\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "        input_ids = inputs['input_ids']\n",
    "        \n",
    "        # Pass the text through the model to get attention scores\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_attentions=True)\n",
    "            attentions = outputs.attentions  # List of attention layers (each contains multiple heads)\n",
    "        \n",
    "        # Get attention scores for the last layer\n",
    "        last_layer_attention = attentions[-1].squeeze(0)  # [num_heads, seq_len, seq_len]\n",
    "        \n",
    "        # Sum attention across heads to get token importance\n",
    "        attention_scores = last_layer_attention.sum(0)  # [seq_len, seq_len]\n",
    "        \n",
    "        # Focus on [CLS] token's attention scores to all other tokens\n",
    "        cls_attention = attention_scores[0]  # Attention to [CLS] token\n",
    "        \n",
    "        # Get tokens and their attention scores\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\n",
    "        \n",
    "        # Rank tokens by their attention score to the [CLS] token\n",
    "        token_attention_pairs = list(zip(tokens, cls_attention.tolist()))\n",
    "        token_attention_pairs = sorted(token_attention_pairs, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Extract top N keywords, filtering out special tokens\n",
    "        keywords = [token for token, score in token_attention_pairs[:top_n] if token not in ['[CLS]', '[SEP]', '[PAD]']]\n",
    "        \n",
    "        # Clean the extracted keywords\n",
    "        return clean_keywords(keywords)\n",
    "\n",
    "    # Apply the extract_keywords function to each row in the financial_texts Series\n",
    "    keywords_series = financial_texts.apply(lambda text: extract_keywords(text, model, tokenizer, top_n))\n",
    "\n",
    "    # Return the Series containing keywords for each document\n",
    "    return keywords_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "ceRFQYMJyLfon0poZsKMRh",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "extract_keywords_series(extracted_df[:5]['2007'], model, tokenizer, top_n=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "flM274LAQh7kIpOPFj13rZ",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#getting keywords for sentiment\n",
    "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_base = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "Sheet 2 Duplicate",
     "sheet_delimiter": true,
     "type": "MD"
    }
   },
   "source": [
    "# Sheet 2 Duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "18EMZ50aHWl9R3O6pC5QxG",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "YQiuMyasf6RSDaUkFtwyeP",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "data_sim = pd.read_csv('/data/workspace_files/lazy_price_replication/10k_final_with_ticker_name_filtered_w_similarity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "CtlECZpITTzQP66RoRWnR2",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "text = data_sim['statement2007'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "G6CZ7gCnfkrJueYgkD61Ef",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "data_sim = data_sim.dropna(subset=['ticker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "1pPqIY3rGVxGaPPA4zdtGX",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "date_columns = [f'file_date{year}' for year in range(2007, 2013)]\n",
    "\n",
    "# Initialize an empty list to store all the filtered dates\n",
    "all_dates = []\n",
    "\n",
    "# Loop through each file_date column\n",
    "for col in date_columns:\n",
    "    filings_date = data_sim[col].to_list()\n",
    "    date_obj = pd.to_datetime(filings_date, format='%Y%m%d', errors='coerce')\n",
    "    all_dates.extend(date_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "ToSydagp4g3hEdyRqaRjY2",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "date_columns = [f'report_period_end_date{year}' for year in range(2007, 2008)]\n",
    "\n",
    "# Initialize an empty list to store all the filtered dates\n",
    "all_dates_rep = []\n",
    "\n",
    "# Loop through each reporting_date column\n",
    "for col in date_columns:\n",
    "    filings_date = data_sim[col].to_list()\n",
    "    date_obj = pd.to_datetime(filings_date, format='%Y%m%d', errors='coerce')\n",
    "    all_dates_rep.extend(date_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "5FCgYQZMtPJkTAPiV5Oy48",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the list of dates into a pandas DataFrame for easier manipulation\n",
    "dates_df = pd.DataFrame(all_dates, columns=['date'])\n",
    "\n",
    "# Add a column for the year and the quarter\n",
    "dates_df['year'] = dates_df['date'].dt.year\n",
    "dates_df['week'] = dates_df['date'].dt.to_period('W')  # This creates values like '2007Q1', '2007Q2', etc.\n",
    "\n",
    "# Count the number of dates in each quarter\n",
    "quarter_counts = dates_df['week'].value_counts().sort_index()\n",
    "\n",
    "# Plot the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "quarter_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title('Number of Filings per week (2007-2012)', fontsize=16)\n",
    "plt.xlabel('week', fontsize=12)\n",
    "plt.ylabel('Number of Dates', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "FwhAOkEMKMx8MYwkFk8Xxh",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the list of dates into a pandas DataFrame for easier manipulation\n",
    "dates_df = pd.DataFrame(all_dates_rep, columns=['date'])\n",
    "\n",
    "# Add a column for the year and the quarter\n",
    "dates_df['year'] = dates_df['date'].dt.year\n",
    "dates_df['week'] = dates_df['date'].dt.to_period('W')  # This creates values like '2007Q1', '2007Q2', etc.\n",
    "\n",
    "# Count the number of dates in each quarter\n",
    "quarter_counts = dates_df['week'].value_counts().sort_index()\n",
    "\n",
    "# Plot the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "quarter_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title('Reporting period per week (2007-2012)', fontsize=16)\n",
    "plt.xlabel('week', fontsize=12)\n",
    "plt.ylabel('Number of Dates', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "i22oeYRw3prJ5rUyI7QthQ",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the years you want to analyze\n",
    "years = [2008, 2009, 2010, 2011, 2012]\n",
    "\n",
    "# Initialize an empty list to store the data dictionaries\n",
    "data_list = []\n",
    "\n",
    "# Loop through each year\n",
    "for year in years:\n",
    "    # Convert the 'file_date' column for the year to datetime format\n",
    "    data_sim[f'file_date{year}'] = pd.to_datetime(data_sim[f'file_date{year}'], format='%Y%m%d')\n",
    "    \n",
    "    # Define the start and end dates for each quarter and calculate trading dates\n",
    "    quarters = [\n",
    "        {'start': f'{year-1}-12-31', 'end': f'{year}-03-31'},  # Q1\n",
    "        {'start': f'{year}-04-01', 'end': f'{year}-06-30'},    # Q2\n",
    "        {'start': f'{year}-07-01', 'end': f'{year}-09-30'},    # Q3\n",
    "        {'start': f'{year}-10-01', 'end': f'{year}-12-31'},    # Q4\n",
    "    ]\n",
    "\n",
    "    # Loop through each quarter for the current year\n",
    "    for quarter in quarters:\n",
    "        start_date = pd.to_datetime(quarter['start'])\n",
    "        end_date = pd.to_datetime(quarter['end'])\n",
    "\n",
    "        # Calculate the trading start and end dates (next quarter)\n",
    "        if quarter['start'] == f'{year-1}-12-31':  # Q1\n",
    "            trading_start = pd.to_datetime(f'{year}-04-01')\n",
    "            trading_end = pd.to_datetime(f'{year}-06-30')\n",
    "            trading_quarter = 'Q2'\n",
    "            trading_year = year\n",
    "        elif quarter['start'] == f'{year}-04-01':  # Q2\n",
    "            trading_start = pd.to_datetime(f'{year}-07-01')\n",
    "            trading_end = pd.to_datetime(f'{year}-09-30')\n",
    "            trading_quarter = 'Q3'\n",
    "            trading_year = year\n",
    "        elif quarter['start'] == f'{year}-07-01':  # Q3\n",
    "            trading_start = pd.to_datetime(f'{year}-10-01')\n",
    "            trading_end = pd.to_datetime(f'{year}-12-31')\n",
    "            trading_quarter = 'Q4'\n",
    "            trading_year = year\n",
    "        else:  # Q4\n",
    "            trading_start = pd.to_datetime(f'{year+1}-01-01')\n",
    "            trading_end = pd.to_datetime(f'{year+1}-03-31')\n",
    "            trading_quarter = 'Q1'\n",
    "            trading_year = year + 1\n",
    "\n",
    "        # Filter the DataFrame for the specified time period\n",
    "        filtered_data = data_sim[(data_sim[f'file_date{year}'] >= start_date) & (data_sim[f'file_date{year}'] <= end_date)]\n",
    "        \n",
    "        # Sort the filtered data by similarity metrics\n",
    "        sorted_data_consine = filtered_data.sort_values(by=f'consine_similarity_{year-1}_to_{year}', ascending=True)\n",
    "        sorted_data_jac = filtered_data.sort_values(by=f'jaccard_similarity_{year-1}_to_{year}', ascending=True)\n",
    "        sorted_data_min_edit = filtered_data.sort_values(by=f'min_edit_distance_similarity_{year-1}_to_{year}', ascending=True)\n",
    "        \n",
    "        # Function to get the top and bottom quintiles\n",
    "        def get_quintiles(sorted_df, col_name):\n",
    "            quintile_size = int(np.ceil(len(sorted_df) * 0.20))\n",
    "            top_quintile = sorted_df.head(quintile_size)[col_name].to_list()\n",
    "            bottom_quintile = sorted_df.tail(quintile_size)[col_name].to_list()\n",
    "            return top_quintile, bottom_quintile\n",
    "\n",
    "        # Get the top and bottom quintile for each similarity measure\n",
    "        top_consine, bottom_consine = get_quintiles(sorted_data_consine, 'ticker')\n",
    "        top_jac, bottom_jac = get_quintiles(sorted_data_jac, 'ticker')\n",
    "        top_min_edit, bottom_min_edit = get_quintiles(sorted_data_min_edit, 'ticker')\n",
    "        # Determine the trading quarter based on the trading_start date\n",
    "\n",
    "        quarter_str = f\"{trading_start.year}-{trading_quarter}\"\n",
    "        # Create a dictionary to store the data for each quarter\n",
    "        data_dict = {\n",
    "            'year': year,\n",
    "            'quarter': quarter_str,\n",
    "            'start_date': start_date,\n",
    "            'end_date': end_date,\n",
    "            'trading_start': trading_start,\n",
    "            'trading_end': trading_end,\n",
    "            'Top Quintile Consine Similarity': top_consine,\n",
    "            'Bottom Quintile Consine Similarity': bottom_consine,\n",
    "            'Top Quintile Jaccard Similarity': top_jac,\n",
    "            'Bottom Quintile Jaccard Similarity': bottom_jac,\n",
    "            'Top Quintile Min Edit Distance Similarity': top_min_edit,\n",
    "            'Bottom Quintile Min Edit Distance Similarity': bottom_min_edit\n",
    "        }\n",
    "\n",
    "        # Append the dictionary to the list\n",
    "        data_list.append(data_dict)\n",
    "\n",
    "# Create a DataFrame from the list of dictionaries\n",
    "result_df = pd.DataFrame(data_list)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "NAMbLjuNXlaM0eogg02xuj",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "price_data = pd.read_csv('/data/workspace_files/lazy_price_replication/all_ticker_prices.csv', index_col=0)\n",
    "\n",
    "# Convert 'Date' to datetime\n",
    "price_data['Date'] = pd.to_datetime(price_data['Date'])\n",
    "\n",
    "# Extract year and quarter from 'Date'\n",
    "price_data['year'] = price_data['Date'].dt.year\n",
    "price_data['quarter'] = price_data['Date'].dt.quarter\n",
    "\n",
    "# Group by ticker, year, and quarter, then calculate quarterly return\n",
    "quarterly_returns = price_data.groupby(['ticker', 'year', 'quarter']).apply(\n",
    "    lambda x: (x['Close'].iloc[-1] - x['Close'].iloc[0]) / x['Close'].iloc[0]\n",
    ").reset_index(name='quarterly_return')\n",
    "\n",
    "# Create a new DataFrame with quarterly periods and returns\n",
    "quarterly_returns['quarter'] = quarterly_returns['year'].astype(str) + '-Q' + quarterly_returns['quarter'].astype(str)\n",
    "\n",
    "# Drop the 'year' column if it's not needed\n",
    "quarterly_prices = quarterly_returns[['ticker', 'quarter', 'quarterly_return']]\n",
    "\n",
    "# Display the new DataFrame\n",
    "price_df = quarterly_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "fUAyp4UKmee2qALDA5vuKF",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming results_df is your stock_df and returns_df is your quarterly_prices\n",
    "\n",
    "# Initialize an empty list to store portfolio returns\n",
    "portfolio_returns = []\n",
    "\n",
    "# Loop through each row of results_df to create the portfolio\n",
    "for index, row in result_df.iterrows():\n",
    "    # Get the tickers for long and short positions\n",
    "    long_stocks = row['Bottom Quintile Jaccard Similarity']\n",
    "    short_stocks = row['Top Quintile Jaccard Similarity']\n",
    "    \n",
    "    # Get the returns for the long stocks\n",
    "    long_returns = price_df[price_df['ticker'].isin(long_stocks) & (price_df['quarter'] == row['quarter'])]['quarterly_return']\n",
    "    \n",
    "    # Get the returns for the short stocks\n",
    "    short_returns = price_df[price_df['ticker'].isin(short_stocks) & (price_df['quarter'] == row['quarter'])]['quarterly_return']\n",
    "    \n",
    "    # Calculate the portfolio return for this quarter\n",
    "    if len(long_returns) > 0 and len(short_returns) > 0:\n",
    "        long_weight = 1 / len(long_stocks)  # Equal weight for long positions\n",
    "        short_weight = -1 / len(short_stocks)  # Equal weight for short positions\n",
    "        \n",
    "        # Portfolio return calculation\n",
    "        portfolio_return = (long_returns.sum() * long_weight) + (short_returns.sum() * short_weight)\n",
    "        portfolio_returns.append({\n",
    "            'quarter': row['quarter'],\n",
    "            'portfolio_return': portfolio_return\n",
    "        })\n",
    "\n",
    "# Create a DataFrame from the portfolio returns\n",
    "portfolio_df = pd.DataFrame(portfolio_returns)\n",
    "\n",
    "# Display the resulting portfolio returns DataFrame\n",
    "print(portfolio_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "fXcfs9ft08N2yrV3M1Uk7A",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get the tickers for long and short positions\n",
    "long_stocks = result_df['Top Quintile Jaccard Similarity'].iloc[0]\n",
    "short_stocks = result_df['Bottom Quintile Jaccard Similarity'].iloc[0]\n",
    "\n",
    "# Get the returns for the long stocks\n",
    "long_returns = price_df[price_df['ticker'].isin(long_stocks) & (price_df['quarter'] == result_df['quarter'].iloc[0])]['quarterly_return']\n",
    "\n",
    "# Get the returns for the short stocks\n",
    "short_returns = price_df[price_df['ticker'].isin(short_stocks) & (price_df['quarter'] == result_df['quarter'].iloc[0])]['quarterly_return']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "82xM5hsjzRCyMFQcA4z3Vc",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "print(long_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "EfErA4B30fAFojASrBh7nI",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample initial investment amount\n",
    "initial_investment = 10000  # or any amount you want\n",
    "\n",
    "# Calculate cumulative returns\n",
    "portfolio_df['cumulative_return'] = (1 + portfolio_df['portfolio_return']).cumprod() - 1\n",
    "\n",
    "# Calculate account value over time\n",
    "portfolio_df['account_value'] = initial_investment * (1 + portfolio_df['cumulative_return'])\n",
    "\n",
    "# Plotting the account value over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(portfolio_df['quarter'], portfolio_df['account_value'], marker='o', linestyle='-', color='blue')\n",
    "plt.title('Portfolio Account Value Over Time')\n",
    "plt.xlabel('Quarter')\n",
    "plt.ylabel('Account Value ($)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "datalore": {
   "base_environment": "default",
   "computation_mode": "JUPYTER",
   "package_manager": "pip",
   "packages": [],
   "report_row_ids": [],
   "version": 3
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "018a4cd8a60b4860b31f13540a3869d9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "169c8e18eaec4c1682620bef283fe52d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "1deb4681aacf4cf6999a291209c09e2e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_57c763a194314d9f80a2b39fb817770b",
       "max": 570,
       "style": "IPY_MODEL_e391734a788f42899b8ac46f4a5f6bcf",
       "value": 570
      }
     },
     "1ea013a2312f40578ad4cbe807d78965": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "2415150767c1463b971f37413e0886db": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_4ea804c963b84dbba02da1888154b1a6",
        "IPY_MODEL_f22511e4bb7a40edb377b7ba22d1e29d",
        "IPY_MODEL_e31cb107677847e69039cf6cbda49036"
       ],
       "layout": "IPY_MODEL_9e0969fc368047f9abe32749d520aa22"
      }
     },
     "3cab5bf6e4d74f4dba785b55750db87a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "4d18a70f12bc4447bf99a70590bea3d7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4e1b50895f124aad8e1d1bb7a237883c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_53eba7d4bccf4086bb1ba0638d4b6c03",
       "max": 213450,
       "style": "IPY_MODEL_693bbd90a7d84fa986e494b676982cc4",
       "value": 213450
      }
     },
     "4ea804c963b84dbba02da1888154b1a6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_df3765ab7bdf4cbab5212c4b0426d139",
       "style": "IPY_MODEL_cd3dd7ad0c344a33a54cd50882ed989d",
       "value": "model.safetensors: 100%"
      }
     },
     "53456c4b962449ec99bc1e185c7c76ac": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "53eba7d4bccf4086bb1ba0638d4b6c03": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "57c763a194314d9f80a2b39fb817770b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5f5f8bf87f7f4b589fee943eb98ff8d8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_a4ec658c5634458ea62d7a079730471b",
        "IPY_MODEL_4e1b50895f124aad8e1d1bb7a237883c",
        "IPY_MODEL_86e8f24b3c3c4b14b6efb00341d66e22"
       ],
       "layout": "IPY_MODEL_018a4cd8a60b4860b31f13540a3869d9"
      }
     },
     "6147215c0863449bbdc4cdf99ec35a09": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "684daf3d62fe418da313a947b7847006": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "693bbd90a7d84fa986e494b676982cc4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "6ab4f7b0608f4ea486daeffa59c21a68": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_684daf3d62fe418da313a947b7847006",
       "style": "IPY_MODEL_3cab5bf6e4d74f4dba785b55750db87a",
       "value": " 49.0/49.0 [00:00&lt;00:00, 1.75kB/s]"
      }
     },
     "80c556d7e5654e369cf112039bd72b5b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "86e8f24b3c3c4b14b6efb00341d66e22": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_53456c4b962449ec99bc1e185c7c76ac",
       "style": "IPY_MODEL_1ea013a2312f40578ad4cbe807d78965",
       "value": " 213k/213k [00:00&lt;00:00, 1.55MB/s]"
      }
     },
     "949a6d11429d41708288faba414862c3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "9891d99389f24b28b08b8eee544bc54e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "992b23182e28471987ce44d8d2e95abf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "9e0969fc368047f9abe32749d520aa22": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a146ca8e8b3e4fd786a75e388280902e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_4d18a70f12bc4447bf99a70590bea3d7",
       "max": 49,
       "style": "IPY_MODEL_949a6d11429d41708288faba414862c3",
       "value": 49
      }
     },
     "a4ec658c5634458ea62d7a079730471b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_80c556d7e5654e369cf112039bd72b5b",
       "style": "IPY_MODEL_b5712f929c634c31a4055ec8cef48377",
       "value": "vocab.txt: 100%"
      }
     },
     "a624f670e2e347d8863cd1177cd0db60": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b3f87a244ffc40a8be7aff8f3b522432": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_d247b0e855c445d1ae738903756b076a",
       "style": "IPY_MODEL_f15772e1f4734a5bb32268bdf95410b1",
       "value": " 570/570 [00:00&lt;00:00, 20.3kB/s]"
      }
     },
     "b5712f929c634c31a4055ec8cef48377": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "c60e5bc4f09b40b0acc071d2a2833232": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_9891d99389f24b28b08b8eee544bc54e",
       "style": "IPY_MODEL_169c8e18eaec4c1682620bef283fe52d",
       "value": "config.json: 100%"
      }
     },
     "cd3dd7ad0c344a33a54cd50882ed989d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "ce4d9e5816af4d818808e455b2655344": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "d247b0e855c445d1ae738903756b076a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "dd99b0f200e54ed68b4d7d5ddd59a254": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_f82db4bb799545f79beddd635752d384",
       "style": "IPY_MODEL_fd64453e562c4ad599037fd3f85b7c79",
       "value": "tokenizer_config.json: 100%"
      }
     },
     "de5d73d271b94a23a761b3ffefb49b06": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_dd99b0f200e54ed68b4d7d5ddd59a254",
        "IPY_MODEL_a146ca8e8b3e4fd786a75e388280902e",
        "IPY_MODEL_6ab4f7b0608f4ea486daeffa59c21a68"
       ],
       "layout": "IPY_MODEL_e946b008f8134ce4aa08dc8aca9cda0c"
      }
     },
     "df3765ab7bdf4cbab5212c4b0426d139": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e31cb107677847e69039cf6cbda49036": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a624f670e2e347d8863cd1177cd0db60",
       "style": "IPY_MODEL_992b23182e28471987ce44d8d2e95abf",
       "value": " 436M/436M [00:01&lt;00:00, 315MB/s]"
      }
     },
     "e391734a788f42899b8ac46f4a5f6bcf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "e4e5e320cc0947cfb30c706f774cc6ac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c60e5bc4f09b40b0acc071d2a2833232",
        "IPY_MODEL_1deb4681aacf4cf6999a291209c09e2e",
        "IPY_MODEL_b3f87a244ffc40a8be7aff8f3b522432"
       ],
       "layout": "IPY_MODEL_e9cc7b27904e44bb992fa90afad0cb78"
      }
     },
     "e946b008f8134ce4aa08dc8aca9cda0c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e9cc7b27904e44bb992fa90afad0cb78": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f15772e1f4734a5bb32268bdf95410b1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "f22511e4bb7a40edb377b7ba22d1e29d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_6147215c0863449bbdc4cdf99ec35a09",
       "max": 435755784,
       "style": "IPY_MODEL_ce4d9e5816af4d818808e455b2655344",
       "value": 435755784
      }
     },
     "f82db4bb799545f79beddd635752d384": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fd64453e562c4ad599037fd3f85b7c79": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
